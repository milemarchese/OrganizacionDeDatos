{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     C:\\Users\\mausa\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\mausa\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\mausa\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\mausa\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import xgboost as xgb\n",
    "import io\n",
    "import nltk\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "from textblob import TextBlob\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "def return_sia_compound_values(text):\n",
    "    return sia.polarity_scores(text)['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(text):\n",
    "    new_text = []\n",
    "    for e in text:\n",
    "        if e not in stopwords and e.isalpha():\n",
    "            new_text.append(e)\n",
    "    text = new_text\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "def stemm(text):\n",
    "    text = [stemmer.stem(word) for word in text.split()]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def contains_punctuation(text):\n",
    "    punctuation = set(string.punctuation)\n",
    "    for character in text:\n",
    "        if character in punctuation:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def amount_of_punctuation(text):\n",
    "    punctuation = set(string.punctuation)\n",
    "    amount = 0\n",
    "    for character in text:\n",
    "        if character in punctuation: amount += 1\n",
    "    return amount\n",
    "\n",
    "def get_adjectives(text):\n",
    "    blob = TextBlob(text)\n",
    "    return len([word for (word,tag) in blob.tags if tag.startswith(\"JJ\")])\n",
    "\n",
    "def get_nouns(text):\n",
    "    blob = TextBlob(text)\n",
    "    return len([word for (word,tag) in blob.tags if tag.startswith(\"NN\")])\n",
    "\n",
    "def get_verbs(text):\n",
    "    blob = TextBlob(text)\n",
    "    return len([word for (word,tag) in blob.tags if tag.startswith(\"VB\")])\n",
    "\n",
    "def get_adverbs(text):\n",
    "    blob = TextBlob(text)\n",
    "    return len([word for (word,tag) in blob.tags if tag.startswith(\"RB\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"train.csv\", usecols=['id','text', 'target'])\n",
    "test = pd.read_csv(\"test.csv\", usecols=['id','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 7434 entries, 0 to 7612\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   id      7434 non-null   int64 \n 1   text    7434 non-null   object\n 2   target  7434 non-null   int64 \ndtypes: int64(2), object(1)\nmemory usage: 232.3+ KB\n"
    }
   ],
   "source": [
    "tweets.drop_duplicates(subset = 'text', keep = False, inplace = True)\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fichur Inginierin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   id                                               text  target  \\\n0   1  Our Deeds are the Reason of this #earthquake M...       1   \n1   4             Forest fire near La Ronge Sask. Canada       1   \n2   5  All residents asked to 'shelter in place' are ...       1   \n3   6  13,000 people receive #wildfires evacuation or...       1   \n4   7  Just got sent this photo from Ruby #Alaska as ...       1   \n\n                              text_without_stopwords  length  avg_word_length  \\\n0              Our Deeds Reason May ALLAH Forgive us      69         4.384615   \n1                   Forest fire near La Ronge Canada      38         4.571429   \n2  All residents asked notified No evacuation she...     133         5.090909   \n3        people receive evacuation orders California      65         7.125000   \n4        Just got sent photo Ruby smoke pours school      88         4.500000   \n\n   amount_of_words  amount_of_unique_words  sentiment  stopwords_count  \\\n0               13                      13     0.2732                6   \n1                7                       7    -0.3400                0   \n2               22                      20    -0.2960               11   \n3                8                       8     0.0000                1   \n4               16                      15     0.0000                7   \n\n   punctuation_count  mentions_count  hashtags_count  \\\n0                  1               0               1   \n1                  1               0               0   \n2                  3               0               0   \n3                  2               0               1   \n4                  2               0               2   \n\n   longest_word_length_without_stopwords  stopword_word_ratio  \\\n0                                      7             0.461538   \n1                                      6             0.000000   \n2                                     10             0.500000   \n3                                     10             0.125000   \n4                                      6             0.437500   \n\n   adjectives_count  nouns_count  verbs_count  adverbs_count  \n0                 0            6            1              0  \n1                 0            6            0              0  \n2                 1            7            7              0  \n3                 1            4            1              0  \n4                 0            6            3              1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>target</th>\n      <th>text_without_stopwords</th>\n      <th>length</th>\n      <th>avg_word_length</th>\n      <th>amount_of_words</th>\n      <th>amount_of_unique_words</th>\n      <th>sentiment</th>\n      <th>stopwords_count</th>\n      <th>punctuation_count</th>\n      <th>mentions_count</th>\n      <th>hashtags_count</th>\n      <th>longest_word_length_without_stopwords</th>\n      <th>stopword_word_ratio</th>\n      <th>adjectives_count</th>\n      <th>nouns_count</th>\n      <th>verbs_count</th>\n      <th>adverbs_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n      <td>Our Deeds Reason May ALLAH Forgive us</td>\n      <td>69</td>\n      <td>4.384615</td>\n      <td>13</td>\n      <td>13</td>\n      <td>0.2732</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>7</td>\n      <td>0.461538</td>\n      <td>0</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n      <td>Forest fire near La Ronge Canada</td>\n      <td>38</td>\n      <td>4.571429</td>\n      <td>7</td>\n      <td>7</td>\n      <td>-0.3400</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n      <td>All residents asked notified No evacuation she...</td>\n      <td>133</td>\n      <td>5.090909</td>\n      <td>22</td>\n      <td>20</td>\n      <td>-0.2960</td>\n      <td>11</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10</td>\n      <td>0.500000</td>\n      <td>1</td>\n      <td>7</td>\n      <td>7</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n      <td>people receive evacuation orders California</td>\n      <td>65</td>\n      <td>7.125000</td>\n      <td>8</td>\n      <td>8</td>\n      <td>0.0000</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>10</td>\n      <td>0.125000</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n      <td>Just got sent photo Ruby smoke pours school</td>\n      <td>88</td>\n      <td>4.500000</td>\n      <td>16</td>\n      <td>15</td>\n      <td>0.0000</td>\n      <td>7</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>6</td>\n      <td>0.437500</td>\n      <td>0</td>\n      <td>6</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "tweets_metrics = tweets[['id','text','target']]\n",
    "tweets_metrics['text_without_stopwords'] = tweets_metrics['text'].str.split()\n",
    "tweets_metrics['text_without_stopwords'] = tweets_metrics['text_without_stopwords'].apply(remove_stopword)\n",
    "\n",
    "tweets_metrics['length'] = tweets_metrics['text'].apply(lambda x: len(x))\n",
    "tweets_metrics['avg_word_length'] = tweets_metrics['text'].str.split().apply(lambda x: [len(y) for y in x]).transform(lambda x: np.mean(x))\n",
    "tweets_metrics['amount_of_words'] = tweets_metrics['text'].str.split().transform(lambda x: len(x))\n",
    "unique_words_by_tweet = tweets_metrics['text'].transform(lambda x: x.split()).transform(lambda x: pd.Series(x).unique()).transform(lambda x: len(x))\n",
    "tweets_metrics['amount_of_unique_words'] = unique_words_by_tweet\n",
    "tweets_metrics['sentiment'] = tweets_metrics['text'].apply(lambda x: return_sia_compound_values(x))\n",
    "tweets_metrics['stopwords_count'] = tweets_metrics['text'].apply(lambda x: len([word for word in str(x).lower().split() if word in stopwords]))\n",
    "tweets_metrics['punctuation_count'] = tweets_metrics['text'].apply(lambda x: amount_of_punctuation(x))\n",
    "mentions = tweets_metrics['text'].str.findall(r'@.\\S*?(?=\\s|[:]|$)').to_frame()\n",
    "tweets_metrics['mentions_count'] = mentions['text'].apply(lambda x: len(x))\n",
    "hashtags = tweets_metrics['text'].str.findall(r'#[^?\\s].*?(?=\\s|$)')\n",
    "tweets_metrics['hashtags_count'] = hashtags.apply(lambda x: len(x))\n",
    "tweets_metrics['longest_word_length_without_stopwords'] = tweets_metrics['text_without_stopwords'].apply(lambda x: ([len(word) for word in str(x).lower().split() if not word.startswith('http')])).apply(lambda x: max(x) if len(x) > 0 else 0)\n",
    "tweets_metrics['stopword_word_ratio'] = tweets_metrics['stopwords_count'] / tweets_metrics['amount_of_words']\n",
    "\n",
    "tweets_metrics['adjectives_count'] = tweets_metrics['text'].apply(get_adjectives)\n",
    "tweets_metrics['nouns_count'] = tweets_metrics['text'].apply(get_nouns)\n",
    "tweets_metrics['verbs_count'] = tweets_metrics['text'].apply(get_verbs)\n",
    "tweets_metrics['adverbs_count'] = tweets_metrics['text'].apply(get_adverbs)\n",
    "\n",
    "tweets_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   id                                               text  \\\n0   0                 Just happened a terrible car crash   \n1   2  Heard about #earthquake is different cities, s...   \n2   3  there is a forest fire at spot pond, geese are...   \n3   9           Apocalypse lighting. #Spokane #wildfires   \n4  11      Typhoon Soudelor kills 28 in China and Taiwan   \n\n                              text_without_stopwords  length  avg_word_length  \\\n0                   Just happened terrible car crash      34         4.833333   \n1                          Heard different stay safe      64         6.222222   \n2  forest fire spot geese fleeing across I cannot...      96         4.105263   \n3                                         Apocalypse      40         9.250000   \n4                Typhoon Soudelor kills China Taiwan      45         4.750000   \n\n   amount_of_words  amount_of_unique_words  sentiment  stopwords_count  \\\n0                6                       6    -0.7003                2   \n1                9                       9     0.4404                2   \n2               19                      19    -0.6159                9   \n3                4                       4     0.0000                0   \n4                8                       8    -0.5423                2   \n\n   punctuation_count  mentions_count  hashtags_count  \\\n0                  0               0               0   \n1                  3               0               1   \n2                  2               0               0   \n3                  3               0               2   \n4                  0               0               0   \n\n   longest_word_length_without_stopwords  stopword_word_ratio  \\\n0                                      8             0.333333   \n1                                      9             0.222222   \n2                                      7             0.473684   \n3                                     10             0.000000   \n4                                      8             0.250000   \n\n   adjectives_count  nouns_count  verbs_count  adverbs_count  \n0                 1            2            1              1  \n1                 2            4            2              0  \n2                 2            4            4              1  \n3                 0            4            0              0  \n4                 0            4            1              0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>text_without_stopwords</th>\n      <th>length</th>\n      <th>avg_word_length</th>\n      <th>amount_of_words</th>\n      <th>amount_of_unique_words</th>\n      <th>sentiment</th>\n      <th>stopwords_count</th>\n      <th>punctuation_count</th>\n      <th>mentions_count</th>\n      <th>hashtags_count</th>\n      <th>longest_word_length_without_stopwords</th>\n      <th>stopword_word_ratio</th>\n      <th>adjectives_count</th>\n      <th>nouns_count</th>\n      <th>verbs_count</th>\n      <th>adverbs_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Just happened a terrible car crash</td>\n      <td>Just happened terrible car crash</td>\n      <td>34</td>\n      <td>4.833333</td>\n      <td>6</td>\n      <td>6</td>\n      <td>-0.7003</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>0.333333</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Heard about #earthquake is different cities, s...</td>\n      <td>Heard different stay safe</td>\n      <td>64</td>\n      <td>6.222222</td>\n      <td>9</td>\n      <td>9</td>\n      <td>0.4404</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>9</td>\n      <td>0.222222</td>\n      <td>2</td>\n      <td>4</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>there is a forest fire at spot pond, geese are...</td>\n      <td>forest fire spot geese fleeing across I cannot...</td>\n      <td>96</td>\n      <td>4.105263</td>\n      <td>19</td>\n      <td>19</td>\n      <td>-0.6159</td>\n      <td>9</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7</td>\n      <td>0.473684</td>\n      <td>2</td>\n      <td>4</td>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>Apocalypse lighting. #Spokane #wildfires</td>\n      <td>Apocalypse</td>\n      <td>40</td>\n      <td>9.250000</td>\n      <td>4</td>\n      <td>4</td>\n      <td>0.0000</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2</td>\n      <td>10</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n      <td>Typhoon Soudelor kills China Taiwan</td>\n      <td>45</td>\n      <td>4.750000</td>\n      <td>8</td>\n      <td>8</td>\n      <td>-0.5423</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>0.250000</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "test = test[['id','text']]\n",
    "test['text_without_stopwords'] = test['text'].str.split()\n",
    "test['text_without_stopwords'] = test['text_without_stopwords'].apply(remove_stopword)\n",
    "\n",
    "test['length'] = test['text'].apply(lambda x: len(x))\n",
    "test['avg_word_length'] = test['text'].str.split().apply(lambda x: [len(y) for y in x]).transform(lambda x: np.mean(x))\n",
    "test['amount_of_words'] = test['text'].str.split().transform(lambda x: len(x))\n",
    "unique_words_by_tweet = test['text'].transform(lambda x: x.split()).transform(lambda x: pd.Series(x).unique()).transform(lambda x: len(x))\n",
    "test['amount_of_unique_words'] = unique_words_by_tweet\n",
    "test['sentiment'] = test['text'].apply(lambda x: return_sia_compound_values(x))\n",
    "test['stopwords_count'] = test['text'].apply(lambda x: len([word for word in str(x).lower().split() if word in stopwords]))\n",
    "test['punctuation_count'] = test['text'].apply(lambda x: amount_of_punctuation(x))\n",
    "mentions = test['text'].str.findall(r'@.\\S*?(?=\\s|[:]|$)').to_frame()\n",
    "test['mentions_count'] = mentions['text'].apply(lambda x: len(x))\n",
    "hashtags = test['text'].str.findall(r'#[^?\\s].*?(?=\\s|$)')\n",
    "test['hashtags_count'] = hashtags.apply(lambda x: len(x))\n",
    "test['longest_word_length_without_stopwords'] = test['text_without_stopwords'].apply(lambda x: ([len(word) for word in str(x).lower().split() if not word.startswith('http')])).apply(lambda x: max(x) if len(x) > 0 else 0)\n",
    "test['stopword_word_ratio'] = test['stopwords_count'] / test['amount_of_words']\n",
    "\n",
    "test['adjectives_count'] = test['text'].apply(get_adjectives)\n",
    "test['nouns_count'] = test['text'].apply(get_nouns)\n",
    "test['verbs_count'] = test['text'].apply(get_verbs)\n",
    "test['adverbs_count'] = test['text'].apply(get_adverbs)\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   length  avg_word_length  amount_of_words  amount_of_unique_words  \\\n0      69         4.384615               13                      13   \n1      38         4.571429                7                       7   \n2     133         5.090909               22                      20   \n3      65         7.125000                8                       8   \n4      88         4.500000               16                      15   \n\n   sentiment  stopwords_count  punctuation_count  mentions_count  \\\n0     0.2732                6                  1               0   \n1    -0.3400                0                  1               0   \n2    -0.2960               11                  3               0   \n3     0.0000                1                  2               0   \n4     0.0000                7                  2               0   \n\n   hashtags_count  longest_word_length_without_stopwords  stopword_word_ratio  \\\n0               1                                      7             0.461538   \n1               0                                      6             0.000000   \n2               0                                     10             0.500000   \n3               1                                     10             0.125000   \n4               2                                      6             0.437500   \n\n   adjectives_count  nouns_count  verbs_count  adverbs_count  \n0                 0            6            1              0  \n1                 0            6            0              0  \n2                 1            7            7              0  \n3                 1            4            1              0  \n4                 0            6            3              1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>length</th>\n      <th>avg_word_length</th>\n      <th>amount_of_words</th>\n      <th>amount_of_unique_words</th>\n      <th>sentiment</th>\n      <th>stopwords_count</th>\n      <th>punctuation_count</th>\n      <th>mentions_count</th>\n      <th>hashtags_count</th>\n      <th>longest_word_length_without_stopwords</th>\n      <th>stopword_word_ratio</th>\n      <th>adjectives_count</th>\n      <th>nouns_count</th>\n      <th>verbs_count</th>\n      <th>adverbs_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>69</td>\n      <td>4.384615</td>\n      <td>13</td>\n      <td>13</td>\n      <td>0.2732</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>7</td>\n      <td>0.461538</td>\n      <td>0</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>38</td>\n      <td>4.571429</td>\n      <td>7</td>\n      <td>7</td>\n      <td>-0.3400</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>133</td>\n      <td>5.090909</td>\n      <td>22</td>\n      <td>20</td>\n      <td>-0.2960</td>\n      <td>11</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10</td>\n      <td>0.500000</td>\n      <td>1</td>\n      <td>7</td>\n      <td>7</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>65</td>\n      <td>7.125000</td>\n      <td>8</td>\n      <td>8</td>\n      <td>0.0000</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>10</td>\n      <td>0.125000</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>88</td>\n      <td>4.500000</td>\n      <td>16</td>\n      <td>15</td>\n      <td>0.0000</td>\n      <td>7</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>6</td>\n      <td>0.437500</td>\n      <td>0</td>\n      <td>6</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "basic_features = tweets_metrics[['length','avg_word_length','amount_of_words','amount_of_unique_words','sentiment','stopwords_count','punctuation_count','mentions_count','hashtags_count','longest_word_length_without_stopwords','stopword_word_ratio','adjectives_count','nouns_count','verbs_count','adverbs_count']]\n",
    "#basic_features = tweets_metrics[['length','avg_word_length','amount_of_words','sentiment', 'stopwords_count', 'punctuation_count', 'longest_word_length_without_stopwords', 'amount_of_unique_words', 'hashtags_count', 'mentions_count']]\n",
    "basic_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   length  avg_word_length  amount_of_words  amount_of_unique_words  \\\n0      34         4.833333                6                       6   \n1      64         6.222222                9                       9   \n2      96         4.105263               19                      19   \n3      40         9.250000                4                       4   \n4      45         4.750000                8                       8   \n\n   sentiment  stopwords_count  punctuation_count  mentions_count  \\\n0    -0.7003                2                  0               0   \n1     0.4404                2                  3               0   \n2    -0.6159                9                  2               0   \n3     0.0000                0                  3               0   \n4    -0.5423                2                  0               0   \n\n   hashtags_count  longest_word_length_without_stopwords  stopword_word_ratio  \\\n0               0                                      8             0.333333   \n1               1                                      9             0.222222   \n2               0                                      7             0.473684   \n3               2                                     10             0.000000   \n4               0                                      8             0.250000   \n\n   adjectives_count  nouns_count  verbs_count  adverbs_count  \n0                 1            2            1              1  \n1                 2            4            2              0  \n2                 2            4            4              1  \n3                 0            4            0              0  \n4                 0            4            1              0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>length</th>\n      <th>avg_word_length</th>\n      <th>amount_of_words</th>\n      <th>amount_of_unique_words</th>\n      <th>sentiment</th>\n      <th>stopwords_count</th>\n      <th>punctuation_count</th>\n      <th>mentions_count</th>\n      <th>hashtags_count</th>\n      <th>longest_word_length_without_stopwords</th>\n      <th>stopword_word_ratio</th>\n      <th>adjectives_count</th>\n      <th>nouns_count</th>\n      <th>verbs_count</th>\n      <th>adverbs_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>34</td>\n      <td>4.833333</td>\n      <td>6</td>\n      <td>6</td>\n      <td>-0.7003</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>0.333333</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>64</td>\n      <td>6.222222</td>\n      <td>9</td>\n      <td>9</td>\n      <td>0.4404</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>9</td>\n      <td>0.222222</td>\n      <td>2</td>\n      <td>4</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>96</td>\n      <td>4.105263</td>\n      <td>19</td>\n      <td>19</td>\n      <td>-0.6159</td>\n      <td>9</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7</td>\n      <td>0.473684</td>\n      <td>2</td>\n      <td>4</td>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>40</td>\n      <td>9.250000</td>\n      <td>4</td>\n      <td>4</td>\n      <td>0.0000</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2</td>\n      <td>10</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>45</td>\n      <td>4.750000</td>\n      <td>8</td>\n      <td>8</td>\n      <td>-0.5423</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>0.250000</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "basic_features_test = test[['length','avg_word_length','amount_of_words','amount_of_unique_words','sentiment','stopwords_count','punctuation_count','mentions_count','hashtags_count','longest_word_length_without_stopwords','stopword_word_ratio','adjectives_count','nouns_count','verbs_count','adverbs_count']]\n",
    "#basic_features_test = test[['length','avg_word_length','amount_of_words','sentiment', 'stopwords_count', 'punctuation_count', 'longest_word_length_without_stopwords', 'amount_of_unique_words', 'hashtags_count', 'mentions_count']]\n",
    "basic_features_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "test = test[['id','text']]\n",
    "\n",
    "x_test_kagle = test['text'].values\n",
    "\n",
    "x_train = tweets_metrics['text'].values\n",
    "y_train = tweets_metrics['target'].values\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 123)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "#x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_test_kagle = tokenizer.texts_to_sequences(x_test_kagle)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "#x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)\n",
    "x_test_kagle = pad_sequences(x_test_kagle, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 100, 100)          2281100   \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 96, 128)           64128     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_9 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 2,346,529\n",
      "Trainable params: 2,346,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "7434/7434 [==============================] - 8s 1ms/step - loss: 0.5909 - accuracy: 0.6987\n",
      "Epoch 2/2\n",
      "7434/7434 [==============================] - 8s 1ms/step - loss: 0.3571 - accuracy: 0.8518\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=2,\n",
    "                    verbose=1,\n",
    "                    #validation_data=(x_test, y_test),\n",
    "                    batch_size=65)\n",
    "#loss, accuracy = model.evaluate(x_train, y_train, verbose=False)\n",
    "#print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "#loss, accuracy = model.evaluate(x_test, y_test, verbose=False)\n",
    "#print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prob</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.879785</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.908730</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.743716</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.566755</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0.961872</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>0.941275</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>0.066874</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>0.047537</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>0.053390</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>0.135765</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id      prob  target\n",
       "0   0  0.879785       1\n",
       "1   2  0.908730       1\n",
       "2   3  0.743716       1\n",
       "3   9  0.566755       1\n",
       "4  11  0.961872       1\n",
       "5  12  0.941275       1\n",
       "6  21  0.066874       0\n",
       "7  22  0.047537       0\n",
       "8  27  0.053390       0\n",
       "9  29  0.135765       0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission1 = pd.DataFrame()\n",
    "submission1['id'] = test['id']\n",
    "submission1['prob'] = model.predict(x_test_kagle)\n",
    "submission1['target'] = submission1['prob'].apply(lambda x: 0 if x < .5 else 1)\n",
    "submission1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "del submission1['prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1\n",
       "5  12       1\n",
       "6  21       0\n",
       "7  22       0\n",
       "8  27       0\n",
       "9  29       0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission1.to_csv(\"submit_prueba_3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una mierda (al final no xd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Luego del submit, mas pruebas con esto (probando con Glove) (dio 0,80570)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#test = pd.read_csv('test.csv')\n",
    "#test = test[['id','text']]\n",
    "\n",
    "#x_test_kagle = test['text'].values\n",
    "\n",
    "#x = tweets_metrics['text'].values\n",
    "#y = tweets_metrics['target'].values\n",
    "x_train = tweets_metrics['text'].values\n",
    "y_train = tweets_metrics['target'].values\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.23, random_state = 123)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "#x_test = tokenizer.texts_to_sequences(x_test)\n",
    "#x_test_kagle = tokenizer.texts_to_sequences(x_test_kagle)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 140\n",
    "\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "#x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)\n",
    "#x_test_kagle = pad_sequences(x_test_kagle, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_dim = 100\n",
    "#embedding_matrix = create_embedding_matrix('glove.6B.200d.txt',tokenizer.word_index, embedding_dim)\n",
    "embedding_matrix = create_embedding_matrix('glove.twitter.27B.100d.txt',tokenizer.word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 140, 100)          2281100   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 134, 128)          89728     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 12)                1548      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 2,372,389\n",
      "Trainable params: 2,372,389\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten\n",
    "from keras import layers\n",
    "#embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=True))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(12, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "7434/7434 [==============================] - 15s 2ms/step - loss: 0.4875 - accuracy: 0.7638\n",
      "Epoch 2/3\n",
      "7434/7434 [==============================] - 14s 2ms/step - loss: 0.3456 - accuracy: 0.8536\n",
      "Epoch 3/3\n",
      "7434/7434 [==============================] - 14s 2ms/step - loss: 0.2587 - accuracy: 0.9015\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "callback = EarlyStopping(monitor = 'val_loss', patience = 1)\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=15,\n",
    "                    verbose=1,\n",
    "                    #validation_data=(x_test, y_test),\n",
    "                    validation_split=0.1,\n",
    "                    batch_size=88,\n",
    "                    callbacks = [callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5724/5724 [==============================] - 2s 349us/step\n",
      "Training Accuracy: 0.9415\n",
      "1710/1710 [==============================] - 1s 352us/step\n",
      "Testing Accuracy:  0.8158, Loss  0.4759\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Testing Accuracy:  {:.4f}, Loss  {:.4f}\".format(accuracy,loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test = test[['id','text']]\n",
    "\n",
    "x_test_kagle = test['text'].values\n",
    "x_test_kagle = tokenizer.texts_to_sequences(x_test_kagle)\n",
    "x_test_kagle = pad_sequences(x_test_kagle, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prob</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.806478</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.946599</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.870620</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.914281</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0.977202</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>0.805770</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>0.018496</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>0.021431</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>0.010347</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>0.072302</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id      prob  target\n",
       "0   0  0.806478       1\n",
       "1   2  0.946599       1\n",
       "2   3  0.870620       1\n",
       "3   9  0.914281       1\n",
       "4  11  0.977202       1\n",
       "5  12  0.805770       1\n",
       "6  21  0.018496       0\n",
       "7  22  0.021431       0\n",
       "8  27  0.010347       0\n",
       "9  29  0.072302       0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission1 = pd.DataFrame()\n",
    "submission1['id'] = test['id']\n",
    "submission1['prob'] = model.predict(x_test_kagle)\n",
    "submission1['target'] = submission1['prob'].apply(lambda x: 0 if x < .5 else 1)\n",
    "submission1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1\n",
       "5  12       1\n",
       "6  21       0\n",
       "7  22       0\n",
       "8  27       0\n",
       "9  29       0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del submission1['prob']\n",
    "submission1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission1.to_csv(\"submit_prueba_14.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras: K fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_filters, kernel_size, vocab_size, embedding_dim, maxlen):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=True))\n",
    "    model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=45 \n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 10s 2ms/step - loss: 0.5141 - accuracy: 0.7556 - val_loss: 0.4774 - val_accuracy: 0.7778\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 10s 2ms/step - loss: 0.3620 - accuracy: 0.8483 - val_loss: 0.4435 - val_accuracy: 0.8029\n",
      "Epoch 3/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.2710 - accuracy: 0.8964 - val_loss: 0.4952 - val_accuracy: 0.7921\n",
      "Epoch 00003: early stopping\n",
      "1859/1859 [==============================] - 0s 257us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=45, total=  29.7s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=45 \n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   29.6s remaining:    0.0s\n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 10s 2ms/step - loss: 0.5259 - accuracy: 0.7588 - val_loss: 0.4598 - val_accuracy: 0.7975\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.3761 - accuracy: 0.8401 - val_loss: 0.4288 - val_accuracy: 0.8029\n",
      "Epoch 3/15\n",
      "5017/5017 [==============================] - 10s 2ms/step - loss: 0.2877 - accuracy: 0.8872 - val_loss: 0.4336 - val_accuracy: 0.8011\n",
      "Epoch 00003: early stopping\n",
      "1859/1859 [==============================] - 0s 262us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=45, total=  29.8s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=45 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 10s 2ms/step - loss: 0.4751 - accuracy: 0.7784 - val_loss: 0.4593 - val_accuracy: 0.7688\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 10s 2ms/step - loss: 0.3191 - accuracy: 0.8699 - val_loss: 0.4573 - val_accuracy: 0.7849\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 10s 2ms/step - loss: 0.2290 - accuracy: 0.9149 - val_loss: 0.4693 - val_accuracy: 0.7814\n",
      "Epoch 00003: early stopping\n",
      "1858/1858 [==============================] - 1s 288us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=45, total=  31.3s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=45 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 10s 2ms/step - loss: 0.5092 - accuracy: 0.7559 - val_loss: 0.4199 - val_accuracy: 0.8172\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 10s 2ms/step - loss: 0.3562 - accuracy: 0.8509 - val_loss: 0.4166 - val_accuracy: 0.8280\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 10s 2ms/step - loss: 0.2612 - accuracy: 0.9022 - val_loss: 0.4358 - val_accuracy: 0.8172\n",
      "Epoch 00003: early stopping\n",
      "1858/1858 [==============================] - 0s 267us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=45, total=  31.0s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=65 \n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.5251 - accuracy: 0.7377 - val_loss: 0.4503 - val_accuracy: 0.7993\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.3499 - accuracy: 0.8543 - val_loss: 0.4259 - val_accuracy: 0.8065\n",
      "Epoch 3/15\n",
      "5017/5017 [==============================] - 10s 2ms/step - loss: 0.2603 - accuracy: 0.9047 - val_loss: 0.4399 - val_accuracy: 0.7921\n",
      "Epoch 00003: early stopping\n",
      "1859/1859 [==============================] - 1s 454us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=65, total=  29.2s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=65 \n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 11s 2ms/step - loss: 0.4991 - accuracy: 0.7654 - val_loss: 0.4462 - val_accuracy: 0.8136\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 12s 2ms/step - loss: 0.3369 - accuracy: 0.8575 - val_loss: 0.4642 - val_accuracy: 0.7832\n",
      "Epoch 00002: early stopping\n",
      "1859/1859 [==============================] - 1s 349us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=65, total=  25.0s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=65 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 12s 2ms/step - loss: 0.5079 - accuracy: 0.7642 - val_loss: 0.4476 - val_accuracy: 0.7993\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 12s 2ms/step - loss: 0.3421 - accuracy: 0.8555 - val_loss: 0.4287 - val_accuracy: 0.8047\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 11s 2ms/step - loss: 0.2454 - accuracy: 0.9073 - val_loss: 0.4657 - val_accuracy: 0.7849\n",
      "Epoch 00003: early stopping\n",
      "1858/1858 [==============================] - 1s 445us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=65, total=  38.1s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=65 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 11s 2ms/step - loss: 0.5054 - accuracy: 0.7615 - val_loss: 0.4291 - val_accuracy: 0.7975\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 11s 2ms/step - loss: 0.3408 - accuracy: 0.8613 - val_loss: 0.4206 - val_accuracy: 0.8082\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 11s 2ms/step - loss: 0.2457 - accuracy: 0.9119 - val_loss: 0.4993 - val_accuracy: 0.8100\n",
      "Epoch 00003: early stopping\n",
      "1858/1858 [==============================] - 1s 424us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=65, total=  34.8s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88 \n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 11s 2ms/step - loss: 0.5406 - accuracy: 0.7297 - val_loss: 0.4604 - val_accuracy: 0.7939\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 11s 2ms/step - loss: 0.3779 - accuracy: 0.8352 - val_loss: 0.4234 - val_accuracy: 0.8154\n",
      "Epoch 3/15\n",
      "5017/5017 [==============================] - 10s 2ms/step - loss: 0.2889 - accuracy: 0.8880 - val_loss: 0.4546 - val_accuracy: 0.7939\n",
      "Epoch 00003: early stopping\n",
      "1859/1859 [==============================] - 1s 354us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88, total=  33.5s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88 \n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 11s 2ms/step - loss: 0.4939 - accuracy: 0.7648 - val_loss: 0.4612 - val_accuracy: 0.7867\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 11s 2ms/step - loss: 0.3503 - accuracy: 0.8495 - val_loss: 0.4238 - val_accuracy: 0.8118\n",
      "Epoch 3/15\n",
      "5017/5017 [==============================] - 12s 2ms/step - loss: 0.2678 - accuracy: 0.9009 - val_loss: 0.4252 - val_accuracy: 0.8190\n",
      "Epoch 00003: early stopping\n",
      "1859/1859 [==============================] - 1s 457us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88, total=  35.3s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 12s 2ms/step - loss: 0.5261 - accuracy: 0.7635 - val_loss: 0.4986 - val_accuracy: 0.7509\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 12s 2ms/step - loss: 0.3573 - accuracy: 0.8539 - val_loss: 0.4417 - val_accuracy: 0.7975\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 11s 2ms/step - loss: 0.2805 - accuracy: 0.8886 - val_loss: 0.4388 - val_accuracy: 0.7993\n",
      "Epoch 4/15\n",
      "5018/5018 [==============================] - 11s 2ms/step - loss: 0.2286 - accuracy: 0.9177 - val_loss: 0.4454 - val_accuracy: 0.7975\n",
      "Epoch 00004: early stopping\n",
      "1858/1858 [==============================] - 1s 403us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88, total=  47.6s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 12s 2ms/step - loss: 0.5251 - accuracy: 0.7453 - val_loss: 0.4337 - val_accuracy: 0.8172\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 11s 2ms/step - loss: 0.3875 - accuracy: 0.8352 - val_loss: 0.3988 - val_accuracy: 0.8441\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 11s 2ms/step - loss: 0.3029 - accuracy: 0.8804 - val_loss: 0.4095 - val_accuracy: 0.8423\n",
      "Epoch 00003: early stopping\n",
      "1858/1858 [==============================] - 1s 424us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88, total=  35.4s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=88 \n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 10s 2ms/step - loss: 0.5044 - accuracy: 0.7562 - val_loss: 0.4389 - val_accuracy: 0.8100\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.3768 - accuracy: 0.8344 - val_loss: 0.4330 - val_accuracy: 0.7993\n",
      "Epoch 3/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.3049 - accuracy: 0.8780 - val_loss: 0.4466 - val_accuracy: 0.7903\n",
      "Epoch 00003: early stopping\n",
      "1859/1859 [==============================] - 0s 262us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=88, total=  29.8s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=88 \n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 7s 1ms/step - loss: 0.4953 - accuracy: 0.7632 - val_loss: 0.4946 - val_accuracy: 0.7652\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 7s 1ms/step - loss: 0.3618 - accuracy: 0.8413 - val_loss: 0.4354 - val_accuracy: 0.8011\n",
      "Epoch 3/15\n",
      "5017/5017 [==============================] - 7s 1ms/step - loss: 0.2861 - accuracy: 0.8890 - val_loss: 0.4320 - val_accuracy: 0.8190\n",
      "Epoch 4/15\n",
      "5017/5017 [==============================] - 7s 1ms/step - loss: 0.2375 - accuracy: 0.9153 - val_loss: 0.4631 - val_accuracy: 0.8082\n",
      "Epoch 00004: early stopping\n",
      "1859/1859 [==============================] - 0s 253us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=88, total=  30.4s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=88 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 8s 2ms/step - loss: 0.5642 - accuracy: 0.6825 - val_loss: 0.4918 - val_accuracy: 0.7527\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 7s 1ms/step - loss: 0.3923 - accuracy: 0.8330 - val_loss: 0.4887 - val_accuracy: 0.7616\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 7s 1ms/step - loss: 0.3194 - accuracy: 0.8677 - val_loss: 0.4350 - val_accuracy: 0.8029\n",
      "Epoch 4/15\n",
      "5018/5018 [==============================] - 7s 1ms/step - loss: 0.2526 - accuracy: 0.9083 - val_loss: 0.4424 - val_accuracy: 0.8029\n",
      "Epoch 00004: early stopping\n",
      "1858/1858 [==============================] - 1s 282us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=88, total=  29.6s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=88 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 7s 1ms/step - loss: 0.5183 - accuracy: 0.7475 - val_loss: 0.4363 - val_accuracy: 0.8208\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 7s 1ms/step - loss: 0.3567 - accuracy: 0.8551 - val_loss: 0.4273 - val_accuracy: 0.8172\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 8s 2ms/step - loss: 0.2758 - accuracy: 0.9033 - val_loss: 0.4326 - val_accuracy: 0.8172\n",
      "Epoch 00003: early stopping\n",
      "1858/1858 [==============================] - 1s 309us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=88, total=  22.6s\n",
      "[CV] vocab_size=22811, num_filters=144, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88 \n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.5139 - accuracy: 0.7441 - val_loss: 0.4678 - val_accuracy: 0.7778\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 8s 2ms/step - loss: 0.3837 - accuracy: 0.8350 - val_loss: 0.4287 - val_accuracy: 0.8136\n",
      "Epoch 3/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.2938 - accuracy: 0.8850 - val_loss: 0.4342 - val_accuracy: 0.8065\n",
      "Epoch 00003: early stopping\n",
      "1859/1859 [==============================] - 1s 349us/step\n",
      "[CV]  vocab_size=22811, num_filters=144, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88, total=  28.0s\n",
      "[CV] vocab_size=22811, num_filters=144, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88 \n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 10s 2ms/step - loss: 0.4898 - accuracy: 0.7688 - val_loss: 0.4453 - val_accuracy: 0.7975\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 10s 2ms/step - loss: 0.3395 - accuracy: 0.8565 - val_loss: 0.5039 - val_accuracy: 0.7545\n",
      "Epoch 00002: early stopping\n",
      "1859/1859 [==============================] - 1s 328us/step\n",
      "[CV]  vocab_size=22811, num_filters=144, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88, total=  21.6s\n",
      "[CV] vocab_size=22811, num_filters=144, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 12s 2ms/step - loss: 0.4900 - accuracy: 0.7654 - val_loss: 0.4399 - val_accuracy: 0.7993\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 10s 2ms/step - loss: 0.3345 - accuracy: 0.8587 - val_loss: 0.4420 - val_accuracy: 0.7957\n",
      "Epoch 00002: early stopping\n",
      "1858/1858 [==============================] - 1s 370us/step\n",
      "[CV]  vocab_size=22811, num_filters=144, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88, total=  23.5s\n",
      "[CV] vocab_size=22811, num_filters=144, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 10s 2ms/step - loss: 0.5100 - accuracy: 0.7505 - val_loss: 0.4274 - val_accuracy: 0.8172\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 11s 2ms/step - loss: 0.3813 - accuracy: 0.8400 - val_loss: 0.4016 - val_accuracy: 0.8405\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 9s 2ms/step - loss: 0.2900 - accuracy: 0.8870 - val_loss: 0.4243 - val_accuracy: 0.8082\n",
      "Epoch 00003: early stopping\n",
      "1858/1858 [==============================] - 1s 365us/step\n",
      "[CV]  vocab_size=22811, num_filters=144, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88, total=  31.3s\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed: 10.3min finished\n",
      "Train on 6690 samples, validate on 744 samples\n",
      "Epoch 1/15\n",
      "6690/6690 [==============================] - 14s 2ms/step - loss: 0.4663 - accuracy: 0.7834 - val_loss: 0.4185 - val_accuracy: 0.8145\n",
      "Epoch 2/15\n",
      "6690/6690 [==============================] - 14s 2ms/step - loss: 0.3259 - accuracy: 0.8646 - val_loss: 0.4480 - val_accuracy: 0.7970\n",
      "Epoch 00002: early stopping\n",
      "Best Accuracy : 0.7942\n",
      "{'vocab_size': 22811, 'num_filters': 128, 'maxlen': 140, 'kernel_size': 5, 'embedding_dim': 100, 'batch_size': 45}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "callback = EarlyStopping(monitor = 'val_loss', patience = 1, verbose=1)\n",
    "\n",
    "x_train = tweets_metrics['text'].values\n",
    "y_train = tweets_metrics['target'].values\n",
    "\n",
    "# Main settings\n",
    "epochs = 15\n",
    "embedding_dim = 100\n",
    "maxlen = 140\n",
    "\n",
    "# Train-test split\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=1, random_state=1000)\n",
    "\n",
    "# Tokenize words\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "#x_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "# Adding 1 because of reserved 0 index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Pad sequences with zeros\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "#x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "embedding_matrix = create_embedding_matrix('glove.twitter.27B.100d.txt',tokenizer.word_index, embedding_dim)\n",
    "\n",
    "# Parameter grid for grid search\n",
    "param_grid = dict(num_filters=[32, 128, 144],\n",
    "                      kernel_size=[3, 5, 7],\n",
    "                      vocab_size=[vocab_size],\n",
    "                      embedding_dim=[embedding_dim],\n",
    "                      maxlen=[maxlen],\n",
    "                      batch_size = [45,65,76,88])\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model,\n",
    "                            epochs=epochs, validation_split=0.1,\n",
    "                            verbose=1)\n",
    "\n",
    "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n",
    "                              cv=4, verbose=2, n_iter=5, n_jobs=1)\n",
    "\n",
    "grid_result = grid.fit(x_train, y_train, callbacks=[callback])\n",
    "\n",
    "# Evaluate testing set\n",
    "#test_accuracy = grid.score(x_test, y_test)\n",
    "\n",
    "# Save and evaluate results\n",
    "s = ('Best Accuracy : {:.4f}\\n{}\\n\\n\\n')\n",
    "output_string = s.format(\n",
    "            grid_result.best_score_,\n",
    "            grid_result.best_params_)\n",
    "            \n",
    "print(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit del 0,81274 con lo extraido del random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#test = pd.read_csv('test.csv')\n",
    "#test = test[['id','text']]\n",
    "\n",
    "#x_test_kagle = test['text'].values\n",
    "\n",
    "#x = tweets_metrics['text'].values\n",
    "#y = tweets_metrics['target'].values\n",
    "x_train = tweets_metrics['text'].values\n",
    "y_train = tweets_metrics['target'].values\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 123)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "#x_test = tokenizer.texts_to_sequences(x_test)\n",
    "#x_test_kagle = tokenizer.texts_to_sequences(x_test_kagle)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 140\n",
    "\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "#x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)\n",
    "#x_test_kagle = pad_sequences(x_test_kagle, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_dim = 100\n",
    "embedding_matrix = create_embedding_matrix('glove.twitter.27B.100d.txt',tokenizer.word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_156\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_156 (Embedding)    (None, 140, 100)          2281100   \n",
      "_________________________________________________________________\n",
      "conv1d_156 (Conv1D)          (None, 134, 128)          89728     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_156 (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_311 (Dense)            (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dense_312 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 2,372,129\n",
      "Trainable params: 2,372,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten\n",
    "from keras import layers\n",
    "#embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=True))\n",
    "model.add(layers.Conv1D(128, 7, activation='relu'))\n",
    "#model.add(layers.Conv1D(32, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "7434/7434 [==============================] - 12s 2ms/step - loss: 0.5673 - accuracy: 0.7171\n",
      "Epoch 2/3\n",
      "7434/7434 [==============================] - 11s 2ms/step - loss: 0.3841 - accuracy: 0.8351\n",
      "Epoch 3/3\n",
      "7434/7434 [==============================] - 12s 2ms/step - loss: 0.2866 - accuracy: 0.8824\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "callback = EarlyStopping(monitor = 'val_loss', patience = 1, verbose=1)\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=3,\n",
    "                    verbose=1,\n",
    "                    #validation_data=(x_test, y_test),\n",
    "                    #validation_split=0.1,\n",
    "                    batch_size=88,\n",
    "                    callbacks = [callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Testing Accuracy:  {:.4f}, Loss  {:.4f}\".format(accuracy,loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test = test[['id','text']]\n",
    "\n",
    "x_test_kagle = test['text'].values\n",
    "x_test_kagle = tokenizer.texts_to_sequences(x_test_kagle)\n",
    "x_test_kagle = pad_sequences(x_test_kagle, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission1 = pd.DataFrame()\n",
    "submission1['id'] = test['id']\n",
    "submission1['prob'] = model.predict(x_test_kagle)\n",
    "submission1['target'] = submission1['prob'].apply(lambda x: 0 if x < .5 else 1)\n",
    "del submission1['prob']\n",
    "submission1.to_csv(\"submit_prueba_13.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas con los features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_features = tweets_metrics[['length','avg_word_length','amount_of_words','amount_of_unique_words','sentiment','stopwords_count','punctuation_count','mentions_count','hashtags_count','longest_word_length_without_stopwords','stopword_word_ratio','adjectives_count','nouns_count','verbs_count','adverbs_count']]\n",
    "#basic_features = tweets_metrics[['length','avg_word_length','amount_of_words','sentiment', 'stopwords_count', 'punctuation_count', 'longest_word_length_without_stopwords', 'amount_of_unique_words', 'hashtags_count', 'mentions_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_features_test = test[['length','avg_word_length','amount_of_words','amount_of_unique_words','sentiment','stopwords_count','punctuation_count','mentions_count','hashtags_count','longest_word_length_without_stopwords','stopword_word_ratio','adjectives_count','nouns_count','verbs_count','adverbs_count']]\n",
    "#basic_features_test = test[['length','avg_word_length','amount_of_words','sentiment', 'stopwords_count', 'punctuation_count', 'longest_word_length_without_stopwords', 'amount_of_unique_words', 'hashtags_count', 'mentions_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Concatenate, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Lambda, Activation, GaussianNoise, GaussianDropout\n",
    "from keras import layers, Input, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv1d(num_filters, kernel_size,vocab_size,embedding_dim,maxlen,batch_size,dense1_size,dense2_size):\n",
    "        \n",
    "    embedding = Embedding(vocab_size, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=True)\n",
    "    x_train_input = Input(shape = (maxlen,), name = 'x_train_input')\n",
    "    x_train_features_input = Input(shape = (15, ), name = 'x_features_train')\n",
    "    emb = embedding(x_train_input)\n",
    "    \n",
    "    conv_out = Conv1D(num_filters, kernel_size, activation='relu')(emb)\n",
    "\n",
    "    max_pool = GlobalMaxPooling1D()(conv_out)\n",
    "    conc = Concatenate()([max_pool, x_train_features_input])\n",
    "    #x = Dropout(0.2)(conc)\n",
    "    \n",
    "    dense1 = Dense(dense1_size, activation='relu')(conc)\n",
    "    dense2 = Dense(dense2_size, activation='relu')(dense1)\n",
    "\n",
    "    #dense3 = Dense(1, activation='sigmoid')(dense2)\n",
    "    \n",
    "    model = Model(inputs = [x_train_input , x_train_features_input], outputs = dense2)\n",
    "    #optimizer = Adam() #default\n",
    "    #model.compile(optimizer=optimizer,\n",
    "              #loss='binary_crossentropy',\n",
    "              #metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_port_to_model(num_filters, kernel_size,vocab_size,embedding_dim,maxlen,batch_size,dense1_size,dense2_size):\n",
    "    combi_input = Input(shape = (155,), name = 'port')\n",
    "    input_train = Lambda(lambda x: x[:,:-15])(combi_input)\n",
    "    input_features = Lambda(lambda x: x[:,140:])(combi_input)\n",
    "\n",
    "    base_network = create_conv1d(num_filters, kernel_size,vocab_size,embedding_dim,maxlen,batch_size,dense1_size,dense2_size)\n",
    "    processed = base_network([input_train,input_features])\n",
    "\n",
    "\n",
    "    dense3 = Dense(1, activation='sigmoid')(processed)\n",
    "    model = Model(combi_input,dense3)\n",
    "    optimizer = Adam() #default\n",
    "    model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = EarlyStopping(monitor = 'val_loss', patience = 1, verbose=1)\n",
    "\n",
    "x_train = tweets_metrics['text'].values\n",
    "y_train = tweets_metrics['target'].values\n",
    "features = StandardScaler()\n",
    "x_train_features = features.fit_transform(basic_features)\n",
    "\n",
    "# Main settings\n",
    "epochs = 15\n",
    "embedding_dim = 100\n",
    "maxlen = 140\n",
    "# Tokenize words\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "\n",
    "embedding_matrix = create_embedding_matrix('glove.twitter.27B.100d.txt',tokenizer.word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Fitting 4 folds for each of 2 candidates, totalling 8 fits\n[CV] batch_size=88, dense1_size=10, dense2_size=20, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811 \n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\nTrain on 5017 samples, validate on 558 samples\nEpoch 1/15\n5017/5017 [==============================] - 9s 2ms/step - loss: 0.5171 - accuracy: 0.7473 - val_loss: 0.5379 - val_accuracy: 0.7509\nEpoch 2/15\n5017/5017 [==============================] - 10s 2ms/step - loss: 0.3783 - accuracy: 0.8385 - val_loss: 0.4833 - val_accuracy: 0.7652\nEpoch 3/15\n5017/5017 [==============================] - 9s 2ms/step - loss: 0.2915 - accuracy: 0.8882 - val_loss: 0.4813 - val_accuracy: 0.7796\nEpoch 4/15\n5017/5017 [==============================] - 8s 2ms/step - loss: 0.2236 - accuracy: 0.9197 - val_loss: 0.4517 - val_accuracy: 0.8047\nEpoch 5/15\n5017/5017 [==============================] - 9s 2ms/step - loss: 0.1452 - accuracy: 0.9569 - val_loss: 0.5113 - val_accuracy: 0.7778\nEpoch 00005: early stopping\n1859/1859 [==============================] - 1s 332us/step\n[CV]  batch_size=88, dense1_size=10, dense2_size=20, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811, total=  46.1s\n[CV] batch_size=88, dense1_size=10, dense2_size=20, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811 \n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   46.0s remaining:    0.0s\nTrain on 5017 samples, validate on 558 samples\nEpoch 1/15\n5017/5017 [==============================] - 8s 2ms/step - loss: 0.5077 - accuracy: 0.7642 - val_loss: 0.5022 - val_accuracy: 0.7616\nEpoch 2/15\n5017/5017 [==============================] - 8s 2ms/step - loss: 0.3717 - accuracy: 0.8399 - val_loss: 0.5288 - val_accuracy: 0.7527\nEpoch 00002: early stopping\n1859/1859 [==============================] - 1s 332us/step\n[CV]  batch_size=88, dense1_size=10, dense2_size=20, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811, total=  18.4s\n[CV] batch_size=88, dense1_size=10, dense2_size=20, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811 \nTrain on 5018 samples, validate on 558 samples\nEpoch 1/15\n5018/5018 [==============================] - 8s 2ms/step - loss: 0.6008 - accuracy: 0.6736 - val_loss: 0.4639 - val_accuracy: 0.7885\nEpoch 2/15\n5018/5018 [==============================] - 8s 2ms/step - loss: 0.3958 - accuracy: 0.8348 - val_loss: 0.4314 - val_accuracy: 0.8100\nEpoch 3/15\n5018/5018 [==============================] - 8s 2ms/step - loss: 0.3038 - accuracy: 0.8786 - val_loss: 0.4457 - val_accuracy: 0.7939\nEpoch 00003: early stopping\n1858/1858 [==============================] - 1s 333us/step\n[CV]  batch_size=88, dense1_size=10, dense2_size=20, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811, total=  26.3s\n[CV] batch_size=88, dense1_size=10, dense2_size=20, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811 \nTrain on 5018 samples, validate on 558 samples\nEpoch 1/15\n5018/5018 [==============================] - 8s 2ms/step - loss: 0.5724 - accuracy: 0.7033 - val_loss: 0.4244 - val_accuracy: 0.8172\nEpoch 2/15\n5018/5018 [==============================] - 8s 2ms/step - loss: 0.4062 - accuracy: 0.8296 - val_loss: 0.4107 - val_accuracy: 0.8423\nEpoch 3/15\n5018/5018 [==============================] - 9s 2ms/step - loss: 0.3247 - accuracy: 0.8762 - val_loss: 0.4219 - val_accuracy: 0.8369\nEpoch 00003: early stopping\n1858/1858 [==============================] - 1s 373us/step\n[CV]  batch_size=88, dense1_size=10, dense2_size=20, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811, total=  26.9s\n[CV] batch_size=88, dense1_size=10, dense2_size=10, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811 \nTrain on 5017 samples, validate on 558 samples\nEpoch 1/15\n5017/5017 [==============================] - 9s 2ms/step - loss: 0.5724 - accuracy: 0.6938 - val_loss: 0.4634 - val_accuracy: 0.7778\nEpoch 2/15\n5017/5017 [==============================] - 9s 2ms/step - loss: 0.4289 - accuracy: 0.8146 - val_loss: 0.4527 - val_accuracy: 0.7903\nEpoch 3/15\n5017/5017 [==============================] - 8s 2ms/step - loss: 0.3472 - accuracy: 0.8585 - val_loss: 0.4449 - val_accuracy: 0.7921\nEpoch 4/15\n5017/5017 [==============================] - 8s 2ms/step - loss: 0.3001 - accuracy: 0.8806 - val_loss: 0.4707 - val_accuracy: 0.7832\nEpoch 00004: early stopping\n1859/1859 [==============================] - 1s 340us/step\n[CV]  batch_size=88, dense1_size=10, dense2_size=10, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811, total=  36.6s\n[CV] batch_size=88, dense1_size=10, dense2_size=10, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811 \nTrain on 5017 samples, validate on 558 samples\nEpoch 1/15\n5017/5017 [==============================] - 9s 2ms/step - loss: 0.5294 - accuracy: 0.7233 - val_loss: 0.4912 - val_accuracy: 0.7581\nEpoch 2/15\n5017/5017 [==============================] - 8s 2ms/step - loss: 0.3692 - accuracy: 0.8417 - val_loss: 0.4598 - val_accuracy: 0.7867\nEpoch 3/15\n5017/5017 [==============================] - 9s 2ms/step - loss: 0.3132 - accuracy: 0.8712 - val_loss: 0.4520 - val_accuracy: 0.8029\nEpoch 4/15\n5017/5017 [==============================] - 9s 2ms/step - loss: 0.2406 - accuracy: 0.9107 - val_loss: 0.4488 - val_accuracy: 0.7921\nEpoch 5/15\n5017/5017 [==============================] - 9s 2ms/step - loss: 0.1823 - accuracy: 0.9386 - val_loss: 0.4679 - val_accuracy: 0.7975\nEpoch 00005: early stopping\n1859/1859 [==============================] - 1s 349us/step\n[CV]  batch_size=88, dense1_size=10, dense2_size=10, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811, total=  45.5s\n[CV] batch_size=88, dense1_size=10, dense2_size=10, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811 \nTrain on 5018 samples, validate on 558 samples\nEpoch 1/15\n5018/5018 [==============================] - 9s 2ms/step - loss: 0.5339 - accuracy: 0.7367 - val_loss: 0.4659 - val_accuracy: 0.7849\nEpoch 2/15\n5018/5018 [==============================] - 8s 2ms/step - loss: 0.3744 - accuracy: 0.8426 - val_loss: 0.4386 - val_accuracy: 0.8100\nEpoch 3/15\n5018/5018 [==============================] - 9s 2ms/step - loss: 0.2799 - accuracy: 0.8880 - val_loss: 0.4639 - val_accuracy: 0.7939\nEpoch 00003: early stopping\n1858/1858 [==============================] - 1s 360us/step\n[CV]  batch_size=88, dense1_size=10, dense2_size=10, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811, total=  27.8s\n[CV] batch_size=88, dense1_size=10, dense2_size=10, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811 \nTrain on 5018 samples, validate on 558 samples\nEpoch 1/15\n5018/5018 [==============================] - 9s 2ms/step - loss: 0.6379 - accuracy: 0.6250 - val_loss: 0.4886 - val_accuracy: 0.8172\nEpoch 2/15\n5018/5018 [==============================] - 9s 2ms/step - loss: 0.4340 - accuracy: 0.8236 - val_loss: 0.4155 - val_accuracy: 0.8280\nEpoch 3/15\n5018/5018 [==============================] - 9s 2ms/step - loss: 0.3304 - accuracy: 0.8743 - val_loss: 0.4099 - val_accuracy: 0.8441\nEpoch 4/15\n5018/5018 [==============================] - 9s 2ms/step - loss: 0.2516 - accuracy: 0.9093 - val_loss: 0.4359 - val_accuracy: 0.8262\nEpoch 00004: early stopping\n1858/1858 [==============================] - 1s 369us/step\n[CV]  batch_size=88, dense1_size=10, dense2_size=10, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811, total=  38.2s\n[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  4.4min finished\nTrain on 6690 samples, validate on 744 samples\nEpoch 1/15\n6690/6690 [==============================] - 12s 2ms/step - loss: 0.5066 - accuracy: 0.7581 - val_loss: 0.4362 - val_accuracy: 0.7944\nEpoch 2/15\n6690/6690 [==============================] - 12s 2ms/step - loss: 0.3665 - accuracy: 0.8472 - val_loss: 0.4244 - val_accuracy: 0.8105\nEpoch 3/15\n6690/6690 [==============================] - 11s 2ms/step - loss: 0.2750 - accuracy: 0.8966 - val_loss: 0.4665 - val_accuracy: 0.7970\nEpoch 00003: early stopping\nBest Accuracy : 0.7854\n{'batch_size': 88, 'dense1_size': 10, 'dense2_size': 10, 'embedding_dim': 100, 'kernel_size': 7, 'maxlen': 140, 'num_filters': 128, 'vocab_size': 22811}\n\n\n\n"
    }
   ],
   "source": [
    "\n",
    "# Parameter grid for grid search\n",
    "param_grid = dict(num_filters=[128],\n",
    "                      kernel_size=[7],\n",
    "                      vocab_size=[vocab_size],\n",
    "                      embedding_dim=[embedding_dim],\n",
    "                      maxlen=[maxlen],\n",
    "                      batch_size = [88],\n",
    "                      dense1_size = [10],\n",
    "                      dense2_size = [20,10])\n",
    "\n",
    "model = KerasClassifier(build_fn=create_port_to_model,\n",
    "                            epochs=epochs, validation_split=0.1,\n",
    "                            verbose=1)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid,\n",
    "                              cv=4, verbose=2, n_jobs=1)\n",
    "\n",
    "grid_result = grid.fit(np.concatenate((x_train,x_train_features), axis = 1), y_train, callbacks=[callback])\n",
    "\n",
    "# Save and evaluate results\n",
    "s = ('Best Accuracy : {:.4f}\\n{}\\n\\n\\n')\n",
    "output_string = s.format(\n",
    "            grid_result.best_score_,\n",
    "            grid_result.best_params_)\n",
    "            \n",
    "print(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n0      28.782677     10.218281         0.639944        0.032818   \n1      36.360590      6.298348         0.661582        0.020972   \n\n  param_batch_size param_dense1_size param_dense2_size param_embedding_dim  \\\n0               88                10                20                 100   \n1               88                10                10                 100   \n\n  param_kernel_size param_maxlen param_num_filters param_vocab_size  \\\n0                 7          140               128            22811   \n1                 7          140               128            22811   \n\n                                              params  split0_test_score  \\\n0  {'batch_size': 88, 'dense1_size': 10, 'dense2_...           0.789672   \n1  {'batch_size': 88, 'dense1_size': 10, 'dense2_...           0.791286   \n\n   split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n0           0.766541           0.771798           0.789020         0.779258   \n1           0.786444           0.778256           0.785791         0.785444   \n\n   std_test_score  rank_test_score  \n0        0.010261                2  \n1        0.004661                1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_batch_size</th>\n      <th>param_dense1_size</th>\n      <th>param_dense2_size</th>\n      <th>param_embedding_dim</th>\n      <th>param_kernel_size</th>\n      <th>param_maxlen</th>\n      <th>param_num_filters</th>\n      <th>param_vocab_size</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>28.782677</td>\n      <td>10.218281</td>\n      <td>0.639944</td>\n      <td>0.032818</td>\n      <td>88</td>\n      <td>10</td>\n      <td>20</td>\n      <td>100</td>\n      <td>7</td>\n      <td>140</td>\n      <td>128</td>\n      <td>22811</td>\n      <td>{'batch_size': 88, 'dense1_size': 10, 'dense2_...</td>\n      <td>0.789672</td>\n      <td>0.766541</td>\n      <td>0.771798</td>\n      <td>0.789020</td>\n      <td>0.779258</td>\n      <td>0.010261</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>36.360590</td>\n      <td>6.298348</td>\n      <td>0.661582</td>\n      <td>0.020972</td>\n      <td>88</td>\n      <td>10</td>\n      <td>10</td>\n      <td>100</td>\n      <td>7</td>\n      <td>140</td>\n      <td>128</td>\n      <td>22811</td>\n      <td>{'batch_size': 88, 'dense1_size': 10, 'dense2_...</td>\n      <td>0.791286</td>\n      <td>0.786444</td>\n      <td>0.778256</td>\n      <td>0.785791</td>\n      <td>0.785444</td>\n      <td>0.004661</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 242
    }
   ],
   "source": [
    "pd.DataFrame(grid_result.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(7434, 155)"
     },
     "metadata": {},
     "execution_count": 199
    }
   ],
   "source": [
    "np.concatenate((x_train,x_train_features), axis = 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(7434, 15)"
     },
     "metadata": {},
     "execution_count": 205
    }
   ],
   "source": [
    "primer_vector = []\n",
    "x_train_concat = np.concatenate((x_train,x_train_features), axis = 1)\n",
    "for array in x_train_concat:\n",
    "    primer_vector.append(array[:-15])\n",
    "segundo_vector = []\n",
    "for i in range(len(primer_vector)):\n",
    "    segundo_vector.append(np.array(primer_vector[i]))\n",
    "x_train_recuperado = np.stack(segundo_vector)\n",
    "\n",
    "tercer_vector = []\n",
    "for array in x_train_concat:\n",
    "    tercer_vector.append(array[140:])\n",
    "cuarto_vector = []\n",
    "for i in range(len(tercer_vector)):\n",
    "    cuarto_vector.append(np.array(tercer_vector[i]))\n",
    "x_train__features_recuperado = np.stack(cuarto_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Concatenate, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Lambda, Activation, GaussianNoise, GaussianDropout\n",
    "from keras import layers, Input, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.constraints import max_norm, unit_norm, min_max_norm\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = StandardScaler()\n",
    "x_train_features = features.fit_transform(basic_features)\n",
    "\n",
    "x_train = tweets_metrics['text'].values\n",
    "y_train = tweets_metrics['target'].values\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "maxlen = 50\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix_840(route, word_index, embedding_dim):\n",
    "    embeddings_index = {}\n",
    "    f = open(route, encoding='utf8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = ''.join(values[:-embedding_dim])\n",
    "        coefs = np.asarray(values[-embedding_dim:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: # Si la palabra no esta queda llena de 0s\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix = create_embedding_matrix('glove.twitter.27B.100d.txt',tokenizer.word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim_1 = 100\n",
    "embedding_matrix_1 = create_embedding_matrix('glove.twitter.27B.100d.txt',tokenizer.word_index, embedding_dim_1)\n",
    "\n",
    "embedding_dim_2 = 300\n",
    "embedding_matrix_2 = create_embedding_matrix_840('glove.840B.300d.txt',tokenizer.word_index, embedding_dim_2)\n",
    "\n",
    "embedding_dim_3 = 200\n",
    "embedding_matrix_3 = create_embedding_matrix('glove.6B.200d.txt',tokenizer.word_index, embedding_dim_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv1d(): #original\n",
    "        \n",
    "    embedding = Embedding(vocab_size, embedding_dim_1, input_length=maxlen, weights=[embedding_matrix_1], trainable=True)\n",
    "    embedding2 = Embedding(vocab_size, embedding_dim_2, input_length=maxlen, weights=[embedding_matrix_2], trainable=True)\n",
    "    embedding3 = Embedding(vocab_size, embedding_dim_3, input_length=maxlen, weights=[embedding_matrix_3], trainable=True)\n",
    "\n",
    "    x_train_input = Input(shape = (maxlen,), name = 'x_train_input')\n",
    "    x_train_features_input = Input(shape = (15, ), name = 'x_features_train')\n",
    "\n",
    "    emb = embedding(x_train_input)\n",
    "    emb2 = embedding2(x_train_input)\n",
    "    emb3 = embedding3(x_train_input)\n",
    "\n",
    "    #Emb 100\n",
    "    conv_out1_1 = Conv1D(128, 2, activation='relu')(emb)\n",
    "    activation1_1 = Activation('relu')(conv_out1_1)\n",
    "    max_pool1_1 = GlobalMaxPooling1D()(activation1_1)\n",
    "    conv_out1_2 = Conv1D(128, 3, activation='relu')(emb)\n",
    "    activation1_2 = Activation('relu')(conv_out1_2)\n",
    "    max_pool1_2 = GlobalMaxPooling1D()(activation1_2)\n",
    "\n",
    "    #Emb 200\n",
    "    conv_out2_1 = Conv1D(128, 2, activation='relu')(emb2)\n",
    "    max_pool2_1 = GlobalMaxPooling1D()(conv_out2_1)\n",
    "    conv_out2_2 = Conv1D(128, 3, activation='relu')(emb2)\n",
    "    max_pool2_2 = GlobalMaxPooling1D()(conv_out2_2)\n",
    "\n",
    "    #Emb 300\n",
    "    conv_out3_1 = Conv1D(128, 2, activation='relu')(emb3)\n",
    "    max_pool3_1 = GlobalMaxPooling1D()(conv_out3_1)\n",
    "    conv_out3_2 = Conv1D(128, 2, activation='relu')(emb3)\n",
    "    max_pool3_2 = GlobalMaxPooling1D()(conv_out3_2)\n",
    "\n",
    "    conc = Concatenate()([max_pool1_1, max_pool1_2, max_pool2_1, max_pool2_2, max_pool3_1, max_pool3_2, x_train_features_input])\n",
    "    \n",
    "    #noise1 = GaussianNoise(0.01)(conc)\n",
    "    dense1 = Dense(100, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01))(conc)\n",
    "    noise2 = GaussianNoise(0.1)(dense1)\n",
    "    \n",
    "    dense2 = Dense(10, activation='relu')(noise2)\n",
    "    #noise1 = GaussianNoise(0.001)(dense2)\n",
    "    dense3 = Dense(1, activation='sigmoid')(dense2)\n",
    "    \n",
    "    model = Model(inputs = [x_train_input , x_train_features_input], outputs = dense3)\n",
    "    optimizer = Adam() #default\n",
    "    model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/3\n7434/7434 [==============================] - 43s 6ms/step - loss: 1.1723 - accuracy: 0.7659\nEpoch 2/3\n7434/7434 [==============================] - 42s 6ms/step - loss: 0.4553 - accuracy: 0.8593\nEpoch 3/3\n7434/7434 [==============================] - 42s 6ms/step - loss: 0.2914 - accuracy: 0.9135\n"
    }
   ],
   "source": [
    "model = create_conv1d()\n",
    "#model.summary()\n",
    "callback = EarlyStopping(monitor = 'val_loss', patience = 1, verbose=1)\n",
    "\n",
    "history = model.fit([x_train,x_train_features], y_train,\n",
    "                    epochs=3,\n",
    "                    verbose=1,\n",
    "                    #validation_data=(x_test, y_test),\n",
    "                    #validation_split=0.2,\n",
    "                    batch_size=64,\n",
    "                    callbacks = [callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.7667790376953724\n"
    }
   ],
   "source": [
    "result = pd.read_csv('perfect_submission.csv')\n",
    "test_kagle = test[['id','text']]\n",
    "\n",
    "x_test_kagle = test['text'].values\n",
    "x_test_kagle = tokenizer.texts_to_sequences(x_test_kagle)\n",
    "x_test_kagle = pad_sequences(x_test_kagle, padding='post', maxlen=maxlen)\n",
    "x_test_features = features.transform(basic_features_test)\n",
    "\n",
    "submit_df = pd.DataFrame()\n",
    "submit_df['id'] = test_kagle['id']\n",
    "submit_df['prob'] = model.predict([x_test_kagle,x_test_features])\n",
    "submit_df['target'] = submit_df['prob'].apply(lambda x: 0 if x < .5 else 1)\n",
    "\n",
    "accuracy = accuracy_score(submit_df['target'], result['target'])\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.820410665032179"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "del submission1['prob']\n",
    "submission1.to_csv(\"submit_prueba_34.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluacin de pre-trained words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings = np.load('glove.840B.300d.pkl', allow_pickle=True)\n",
    "fasttext_embeddings = np.load('crawl-300d-2M.pkl', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "GloVe Embeddings cover 52.17% of vocabulary and 82.73% of text in Training Set\nGloVe Embeddings cover 57.21% of vocabulary and 81.85% of text in Test Set\nFastText Embeddings cover 51.63% of vocabulary and 81.88% of text in Training Set\nFastText Embeddings cover 56.55% of vocabulary and 81.12% of text in Test Set\n"
    }
   ],
   "source": [
    "import operator\n",
    "def build_vocab(X):\n",
    "    \n",
    "    tweets = X.apply(lambda s: s.split()).values      \n",
    "    vocab = {}\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        for word in tweet:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1                \n",
    "    return vocab\n",
    "\n",
    "\n",
    "def check_embeddings_coverage(X, embeddings):\n",
    "    \n",
    "    vocab = build_vocab(X)    \n",
    "    \n",
    "    covered = {}\n",
    "    oov = {}    \n",
    "    n_covered = 0\n",
    "    n_oov = 0\n",
    "    \n",
    "    for word in vocab:\n",
    "        try:\n",
    "            covered[word] = embeddings[word]\n",
    "            n_covered += vocab[word]\n",
    "        except:\n",
    "            oov[word] = vocab[word]\n",
    "            n_oov += vocab[word]\n",
    "            \n",
    "    vocab_coverage = len(covered) / len(vocab)\n",
    "    text_coverage = (n_covered / (n_covered + n_oov))\n",
    "    \n",
    "    sorted_oov = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "    return sorted_oov, vocab_coverage, text_coverage\n",
    "\n",
    "train_glove_oov, train_glove_vocab_coverage, train_glove_text_coverage = check_embeddings_coverage(tweets_metrics['text'], glove_embeddings)\n",
    "test_glove_oov, test_glove_vocab_coverage, test_glove_text_coverage = check_embeddings_coverage(test['text'], glove_embeddings)\n",
    "print('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(train_glove_vocab_coverage, train_glove_text_coverage))\n",
    "print('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set'.format(test_glove_vocab_coverage, test_glove_text_coverage))\n",
    "\n",
    "train_fasttext_oov, train_fasttext_vocab_coverage, train_fasttext_text_coverage = check_embeddings_coverage(tweets_metrics['text'], fasttext_embeddings)\n",
    "test_fasttext_oov, test_fasttext_vocab_coverage, test_fasttext_text_coverage = check_embeddings_coverage(test['text'], fasttext_embeddings)\n",
    "print('FastText Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(train_fasttext_vocab_coverage, train_fasttext_text_coverage))\n",
    "print('FastText Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set'.format(test_fasttext_vocab_coverage, test_fasttext_text_coverage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.5652974442155101"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "nonzero_elements / vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('mausa': virtualenv)",
   "language": "python",
   "name": "python37664bitmausavirtualenv161e16f0fd2c481895212b5856f17cf5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}