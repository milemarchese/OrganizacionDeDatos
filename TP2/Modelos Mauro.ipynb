{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\mausa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mausa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mausa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mausa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import xgboost as xgb\n",
    "import io\n",
    "import nltk\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "from textblob import TextBlob\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"train.csv\", usecols=['id','text', 'target'])\n",
    "test = pd.read_csv(\"test.csv\", usecols=['id','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7434 entries, 0 to 7612\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      7434 non-null   int64 \n",
      " 1   text    7434 non-null   object\n",
      " 2   target  7434 non-null   int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 232.3+ KB\n"
     ]
    }
   ],
   "source": [
    "tweets.drop_duplicates(subset = 'text', keep = False, inplace = True)\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w_features = pd.read_csv('../TP2/train_features.csv')\n",
    "test_w_features = pd.read_csv('../TP2/test_features.csv')\n",
    "keyword_features = pd.read_csv('../TP2/keyword_features.csv')\n",
    "keyword_test_features = pd.read_csv('../TP2/keyword_test_features.csv')\n",
    "train_processed_text = pd.read_csv('../TP2/processed_train.csv')\n",
    "test_processed_text = pd.read_csv('../TP2/processed_test.csv')\n",
    "\n",
    "train_w_features.insert(3,'target',train_processed_text['target'])\n",
    "train_features_and_kw = train_w_features.merge(keyword_features, on='id')\n",
    "train_features_and_kw.insert(3,'processed_text', train_processed_text['text'])\n",
    "\n",
    "test_features_and_kw = test_w_features.merge(keyword_test_features, on='id')\n",
    "test_features_and_kw.insert(3,'processed_text', test_processed_text['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "    return embedding_matrix\n",
    "\n",
    "def create_embedding_matrix_840(route, word_index, embedding_dim):\n",
    "    embeddings_index = {}\n",
    "    f = open(route, encoding='utf8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = ''.join(values[:-embedding_dim])\n",
    "        coefs = np.asarray(values[-embedding_dim:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: # Si la palabra no esta queda llena de 0s\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Concatenate, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Lambda, Activation, GaussianNoise, GaussianDropout\n",
    "from keras import layers, Input, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.constraints import max_norm, unit_norm, min_max_norm\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def create_conv1d(num_filters, kernel_size,vocab_size,embedding_dim,maxlen,batch_size,dense1_size,dense2_size):\n",
    "    embedding = Embedding(vocab_size, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=True)\n",
    "    x_train_input = Input(shape = (maxlen,), name = 'x_train_input')\n",
    "    x_train_features_input = Input(shape = (15, ), name = 'x_features_train')\n",
    "    emb = embedding(x_train_input)\n",
    "    \n",
    "    conv_out = Conv1D(num_filters, kernel_size, activation='relu')(emb)\n",
    "    max_pool = GlobalMaxPooling1D()(conv_out)\n",
    "    \n",
    "    conc = Concatenate()([max_pool, x_train_features_input])\n",
    "   \n",
    "    dense1 = Dense(dense1_size, activation='relu')(conc)\n",
    "    dense2 = Dense(dense2_size, activation='relu')(dense1)\n",
    "    \n",
    "    model = Model(inputs = [x_train_input , x_train_features_input], outputs = dense2)\n",
    "    return model\n",
    "\n",
    "def create_port_to_model(num_filters, kernel_size,vocab_size,embedding_dim,maxlen,batch_size,dense1_size,dense2_size):\n",
    "    combi_input = Input(shape = (155,), name = 'port')\n",
    "    input_train = Lambda(lambda x: x[:,:-15])(combi_input)\n",
    "    input_features = Lambda(lambda x: x[:,140:])(combi_input)\n",
    "\n",
    "    base_network = create_conv1d(num_filters, kernel_size,vocab_size,embedding_dim,maxlen,batch_size,dense1_size,dense2_size)\n",
    "    processed = base_network([input_train,input_features])\n",
    "\n",
    "    dense3 = Dense(1, activation='sigmoid')(processed)\n",
    "    model = Model(combi_input,dense3)\n",
    "    optimizer = Adam() #default\n",
    "    model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "callback = EarlyStopping(monitor = 'val_loss', patience = 1, verbose=1)\n",
    "\n",
    "x_train = tweets_metrics['text'].values\n",
    "y_train = tweets_metrics['target'].values\n",
    "features = StandardScaler()\n",
    "x_train_features = features.fit_transform(basic_features)\n",
    "\n",
    "epochs = 15\n",
    "embedding_dim = 100\n",
    "maxlen = 140\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "\n",
    "embedding_matrix = create_embedding_matrix('Embeddings/glove.twitter.27B.100d.txt',tokenizer.word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 2 candidates, totalling 8 fits\n",
      "[CV] batch_size=88, dense1_size=10, dense2_size=20, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811 \n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.5171 - accuracy: 0.7473 - val_loss: 0.5379 - val_accuracy: 0.7509\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 10s 2ms/step - loss: 0.3783 - accuracy: 0.8385 - val_loss: 0.4833 - val_accuracy: 0.7652\n",
      "Epoch 3/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.2915 - accuracy: 0.8882 - val_loss: 0.4813 - val_accuracy: 0.7796\n",
      "Epoch 4/15\n",
      "5017/5017 [==============================] - 8s 2ms/step - loss: 0.2236 - accuracy: 0.9197 - val_loss: 0.4517 - val_accuracy: 0.8047\n",
      "Epoch 5/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.1452 - accuracy: 0.9569 - val_loss: 0.5113 - val_accuracy: 0.7778\n",
      "Epoch 00005: early stopping\n",
      "1859/1859 [==============================] - 1s 332us/step\n",
      "[CV]  batch_size=88, dense1_size=10, dense2_size=20, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811, total=  46.1s\n",
      "[CV] batch_size=88, dense1_size=10, dense2_size=20, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811 \n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   46.0s remaining:    0.0s\n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 8s 2ms/step - loss: 0.5077 - accuracy: 0.7642 - val_loss: 0.5022 - val_accuracy: 0.7616\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 8s 2ms/step - loss: 0.3717 - accuracy: 0.8399 - val_loss: 0.5288 - val_accuracy: 0.7527\n",
      "Epoch 00002: early stopping\n",
      "1859/1859 [==============================] - 1s 332us/step\n",
      "[CV]  batch_size=88, dense1_size=10, dense2_size=20, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811, total=  18.4s\n",
      "[CV] batch_size=88, dense1_size=10, dense2_size=20, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 8s 2ms/step - loss: 0.6008 - accuracy: 0.6736 - val_loss: 0.4639 - val_accuracy: 0.7885\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 8s 2ms/step - loss: 0.3958 - accuracy: 0.8348 - val_loss: 0.4314 - val_accuracy: 0.8100\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 8s 2ms/step - loss: 0.3038 - accuracy: 0.8786 - val_loss: 0.4457 - val_accuracy: 0.7939\n",
      "Epoch 00003: early stopping\n",
      "1858/1858 [==============================] - 1s 333us/step\n",
      "[CV]  batch_size=88, dense1_size=10, dense2_size=20, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811, total=  26.3s\n",
      "[CV] batch_size=88, dense1_size=10, dense2_size=20, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 8s 2ms/step - loss: 0.5724 - accuracy: 0.7033 - val_loss: 0.4244 - val_accuracy: 0.8172\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 8s 2ms/step - loss: 0.4062 - accuracy: 0.8296 - val_loss: 0.4107 - val_accuracy: 0.8423\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 9s 2ms/step - loss: 0.3247 - accuracy: 0.8762 - val_loss: 0.4219 - val_accuracy: 0.8369\n",
      "Epoch 00003: early stopping\n",
      "1858/1858 [==============================] - 1s 373us/step\n",
      "[CV]  batch_size=88, dense1_size=10, dense2_size=20, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811, total=  26.9s\n",
      "[CV] batch_size=88, dense1_size=10, dense2_size=10, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811 \n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.5724 - accuracy: 0.6938 - val_loss: 0.4634 - val_accuracy: 0.7778\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.4289 - accuracy: 0.8146 - val_loss: 0.4527 - val_accuracy: 0.7903\n",
      "Epoch 3/15\n",
      "5017/5017 [==============================] - 8s 2ms/step - loss: 0.3472 - accuracy: 0.8585 - val_loss: 0.4449 - val_accuracy: 0.7921\n",
      "Epoch 4/15\n",
      "5017/5017 [==============================] - 8s 2ms/step - loss: 0.3001 - accuracy: 0.8806 - val_loss: 0.4707 - val_accuracy: 0.7832\n",
      "Epoch 00004: early stopping\n",
      "1859/1859 [==============================] - 1s 340us/step\n",
      "[CV]  batch_size=88, dense1_size=10, dense2_size=10, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811, total=  36.6s\n",
      "[CV] batch_size=88, dense1_size=10, dense2_size=10, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811 \n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.5294 - accuracy: 0.7233 - val_loss: 0.4912 - val_accuracy: 0.7581\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 8s 2ms/step - loss: 0.3692 - accuracy: 0.8417 - val_loss: 0.4598 - val_accuracy: 0.7867\n",
      "Epoch 3/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.3132 - accuracy: 0.8712 - val_loss: 0.4520 - val_accuracy: 0.8029\n",
      "Epoch 4/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.2406 - accuracy: 0.9107 - val_loss: 0.4488 - val_accuracy: 0.7921\n",
      "Epoch 5/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.1823 - accuracy: 0.9386 - val_loss: 0.4679 - val_accuracy: 0.7975\n",
      "Epoch 00005: early stopping\n",
      "1859/1859 [==============================] - 1s 349us/step\n",
      "[CV]  batch_size=88, dense1_size=10, dense2_size=10, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811, total=  45.5s\n",
      "[CV] batch_size=88, dense1_size=10, dense2_size=10, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 9s 2ms/step - loss: 0.5339 - accuracy: 0.7367 - val_loss: 0.4659 - val_accuracy: 0.7849\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 8s 2ms/step - loss: 0.3744 - accuracy: 0.8426 - val_loss: 0.4386 - val_accuracy: 0.8100\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 9s 2ms/step - loss: 0.2799 - accuracy: 0.8880 - val_loss: 0.4639 - val_accuracy: 0.7939\n",
      "Epoch 00003: early stopping\n",
      "1858/1858 [==============================] - 1s 360us/step\n",
      "[CV]  batch_size=88, dense1_size=10, dense2_size=10, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811, total=  27.8s\n",
      "[CV] batch_size=88, dense1_size=10, dense2_size=10, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 9s 2ms/step - loss: 0.6379 - accuracy: 0.6250 - val_loss: 0.4886 - val_accuracy: 0.8172\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 9s 2ms/step - loss: 0.4340 - accuracy: 0.8236 - val_loss: 0.4155 - val_accuracy: 0.8280\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 9s 2ms/step - loss: 0.3304 - accuracy: 0.8743 - val_loss: 0.4099 - val_accuracy: 0.8441\n",
      "Epoch 4/15\n",
      "5018/5018 [==============================] - 9s 2ms/step - loss: 0.2516 - accuracy: 0.9093 - val_loss: 0.4359 - val_accuracy: 0.8262\n",
      "Epoch 00004: early stopping\n",
      "1858/1858 [==============================] - 1s 369us/step\n",
      "[CV]  batch_size=88, dense1_size=10, dense2_size=10, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811, total=  38.2s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  4.4min finished\n",
      "Train on 6690 samples, validate on 744 samples\n",
      "Epoch 1/15\n",
      "6690/6690 [==============================] - 12s 2ms/step - loss: 0.5066 - accuracy: 0.7581 - val_loss: 0.4362 - val_accuracy: 0.7944\n",
      "Epoch 2/15\n",
      "6690/6690 [==============================] - 12s 2ms/step - loss: 0.3665 - accuracy: 0.8472 - val_loss: 0.4244 - val_accuracy: 0.8105\n",
      "Epoch 3/15\n",
      "6690/6690 [==============================] - 11s 2ms/step - loss: 0.2750 - accuracy: 0.8966 - val_loss: 0.4665 - val_accuracy: 0.7970\n",
      "Epoch 00003: early stopping\n",
      "Best Accuracy : 0.7854\n",
      "{'batch_size': 88, 'dense1_size': 10, 'dense2_size': 10, 'embedding_dim': 100, 'kernel_size': 7, 'maxlen': 140, 'num_filters': 128, 'vocab_size': 22811}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_grid = dict(num_filters=[128],\n",
    "                      kernel_size=[7],\n",
    "                      vocab_size=[vocab_size],\n",
    "                      embedding_dim=[embedding_dim],\n",
    "                      maxlen=[maxlen],\n",
    "                      batch_size = [88],\n",
    "                      dense1_size = [10],\n",
    "                      dense2_size = [20,10])\n",
    "\n",
    "model = KerasClassifier(build_fn=create_port_to_model,\n",
    "                            epochs=epochs, validation_split=0.1,\n",
    "                            verbose=1)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid,\n",
    "                              cv=4, verbose=2, n_jobs=1)\n",
    "\n",
    "grid_result = grid.fit(np.concatenate((x_train,x_train_features), axis = 1), y_train, callbacks=[callback])\n",
    "\n",
    "s = ('Best Accuracy : {:.4f}\\n{}\\n\\n\\n')\n",
    "output_string = s.format(\n",
    "            grid_result.best_score_,\n",
    "            grid_result.best_params_)\n",
    "print(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_batch_size</th>\n",
       "      <th>param_dense1_size</th>\n",
       "      <th>param_dense2_size</th>\n",
       "      <th>param_embedding_dim</th>\n",
       "      <th>param_kernel_size</th>\n",
       "      <th>param_maxlen</th>\n",
       "      <th>param_num_filters</th>\n",
       "      <th>param_vocab_size</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.782677</td>\n",
       "      <td>10.218281</td>\n",
       "      <td>0.639944</td>\n",
       "      <td>0.032818</td>\n",
       "      <td>88</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "      <td>7</td>\n",
       "      <td>140</td>\n",
       "      <td>128</td>\n",
       "      <td>22811</td>\n",
       "      <td>{'batch_size': 88, 'dense1_size': 10, 'dense2_...</td>\n",
       "      <td>0.789672</td>\n",
       "      <td>0.766541</td>\n",
       "      <td>0.771798</td>\n",
       "      <td>0.789020</td>\n",
       "      <td>0.779258</td>\n",
       "      <td>0.010261</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36.360590</td>\n",
       "      <td>6.298348</td>\n",
       "      <td>0.661582</td>\n",
       "      <td>0.020972</td>\n",
       "      <td>88</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>7</td>\n",
       "      <td>140</td>\n",
       "      <td>128</td>\n",
       "      <td>22811</td>\n",
       "      <td>{'batch_size': 88, 'dense1_size': 10, 'dense2_...</td>\n",
       "      <td>0.791286</td>\n",
       "      <td>0.786444</td>\n",
       "      <td>0.778256</td>\n",
       "      <td>0.785791</td>\n",
       "      <td>0.785444</td>\n",
       "      <td>0.004661</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      28.782677     10.218281         0.639944        0.032818   \n",
       "1      36.360590      6.298348         0.661582        0.020972   \n",
       "\n",
       "  param_batch_size param_dense1_size param_dense2_size param_embedding_dim  \\\n",
       "0               88                10                20                 100   \n",
       "1               88                10                10                 100   \n",
       "\n",
       "  param_kernel_size param_maxlen param_num_filters param_vocab_size  \\\n",
       "0                 7          140               128            22811   \n",
       "1                 7          140               128            22811   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'batch_size': 88, 'dense1_size': 10, 'dense2_...           0.789672   \n",
       "1  {'batch_size': 88, 'dense1_size': 10, 'dense2_...           0.791286   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
       "0           0.766541           0.771798           0.789020         0.779258   \n",
       "1           0.786444           0.778256           0.785791         0.785444   \n",
       "\n",
       "   std_test_score  rank_test_score  \n",
       "0        0.010261                2  \n",
       "1        0.004661                1  "
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid_result.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_and_kw['text_without_stopwords'] = train_features_and_kw['text_without_stopwords'].fillna('')\n",
    "test_features_and_kw['text_without_stopwords'] = test_features_and_kw['text_without_stopwords'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Concatenate, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Lambda, Activation, GaussianNoise, GaussianDropout\n",
    "from keras import layers, Input, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.constraints import max_norm, unit_norm, min_max_norm\n",
    "from tensorflow import keras\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train_features = scaler.fit_transform(train_features_and_kw.loc[:,'length':])\n",
    "\n",
    "x_train = train_features_and_kw['processed_text'].values\n",
    "y_train = train_features_and_kw['target'].values\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "maxlen = 50\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv('perfect_submission.csv')\n",
    "test_kagle = test_features_and_kw[['id','text']]\n",
    "\n",
    "x_test_kagle = test_features_and_kw['processed_text'].values\n",
    "x_test_kagle = tokenizer.texts_to_sequences(x_test_kagle)\n",
    "x_test_kagle = pad_sequences(x_test_kagle, padding='post', maxlen=maxlen)\n",
    "x_test_features = scaler.transform(test_features_and_kw.loc[:,'length':])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim_1 = 100\n",
    "embedding_matrix_1 = create_embedding_matrix('Embeddings/glove.twitter.27B.100d.txt',tokenizer.word_index, embedding_dim_1)\n",
    "\n",
    "#embedding_dim_2 = 300\n",
    "#embedding_matrix_2 = create_embedding_matrix_840('Embeddings/glove.840B.300d.txt',tokenizer.word_index, embedding_dim_2)\n",
    "\n",
    "#embedding_dim_3 = 200\n",
    "#embedding_matrix_3 = create_embedding_matrix('Embeddings/glove.6B.200d.txt',tokenizer.word_index, embedding_dim_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv1d(dense_1=130,conv1=256,conv2=128): #original\n",
    "        \n",
    "    embedding = Embedding(vocab_size, embedding_dim_1, input_length=maxlen, weights=[embedding_matrix_1], trainable=True)\n",
    "\n",
    "    x_train_input = Input(shape = (maxlen,), name = 'x_train_input')\n",
    "    x_train_features_input = Input(shape = (115, ), name = 'x_features_train')\n",
    "\n",
    "    emb = embedding(x_train_input)\n",
    "    #conv_out2_1 = Conv1D(128, 2, activation='relu', kernel_constraint=max_norm(3), bias_constraint=max_norm(3))(emb2)\n",
    "    \n",
    "    conv_out1 = Conv1D(512, 2, activation='relu')(emb)\n",
    "    activation1 = Activation('relu')(conv_out1)\n",
    "    max_pool1 = GlobalMaxPooling1D()(activation1)\n",
    "    conv_out2 = Conv1D(256, 2, activation='relu')(emb)\n",
    "    activation2 = Activation('relu')(conv_out2)\n",
    "    max_pool2 = GlobalMaxPooling1D()(activation2)\n",
    "\n",
    "    conc = Concatenate()([max_pool2, max_pool1, x_train_features_input])\n",
    "    \n",
    "    #dense1 = Dense(28, activation='relu')(conc)\n",
    "    #noise2 = GaussianNoise(0.15)(dense1)\n",
    "    #drop = Dropout(0.3)(dense1)\n",
    "    \n",
    "    dense2 = Dense(dense_1, activation='relu')(conc)\n",
    "    dense3 = Dense(1, activation='sigmoid')(dense2)\n",
    "    \n",
    "    model = Model(inputs = [x_train_input , x_train_features_input], outputs = dense3)\n",
    "    optimizer = Adam() #default\n",
    "    model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "7434/7434 [==============================] - 11s 1ms/step - loss: 0.4681 - accuracy: 0.7841\n",
      "Epoch 2/2\n",
      "7434/7434 [==============================] - 11s 1ms/step - loss: 0.3465 - accuracy: 0.8506\n"
     ]
    }
   ],
   "source": [
    "model = create_conv1d()\n",
    "history = model.fit([x_train,x_train_features], y_train,\n",
    "                    epochs=2,\n",
    "                    verbose=1,\n",
    "                    batch_size=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8102972724486669\n"
     ]
    }
   ],
   "source": [
    "submit_df = pd.DataFrame()\n",
    "submit_df['id'] = test_kagle['id']\n",
    "submit_df['prob'] = model.predict([x_test_kagle,x_test_features])\n",
    "submit_df['target'] = submit_df['prob'].apply(lambda x: 0 if x < .5 else 1)\n",
    "\n",
    "accuracy = accuracy_score(submit_df['target'], result['target'])\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.82531"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "del submit_df['prob']\n",
    "submit_df.to_csv(\"submit_prueba_47.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capa Densa Cantidad: 112 - Score: 0.8213300643579527\n",
      "Capa Densa Cantidad: 162 - Score: 0.8207171314741036\n",
      "Capa Densa Cantidad: 247 - Score: 0.820410665032179\n",
      "Capa Densa Cantidad: 287 - Score: 0.8213300643579527\n",
      "Capa Densa Cantidad: 292 - Score: 0.820410665032179\n",
      "Capa Densa Cantidad: 297 - Score: 0.8222494636837266\n"
     ]
    }
   ],
   "source": [
    "for i in range(2,300,5):\n",
    "    model = create_conv1d(130,i,i)\n",
    "    history = model.fit([x_train,x_train_features], y_train,\n",
    "                    epochs=2,\n",
    "                    verbose=0,\n",
    "                    batch_size=69)\n",
    "    submit_df = pd.DataFrame()\n",
    "    submit_df['id'] = test_kagle['id']\n",
    "    submit_df['prob'] = model.predict([x_test_kagle,x_test_features])\n",
    "    submit_df['target'] = submit_df['prob'].apply(lambda x: 0 if x < .5 else 1)\n",
    "\n",
    "    accuracy = accuracy_score(submit_df['target'], result['target'])\n",
    "    if accuracy > 0.82:\n",
    "        print(f'Capa Densa Cantidad: {i} - Score: {accuracy}')\n",
    "    if accuracy > 0.8257:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificaci√≥n de misslabeled tweets individuales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "pa_checkear = test_w_features[['id','text']]\n",
    "pa_checkear['target'] = result['target']\n",
    "with open('perfect_submission.csv') as perfecto, open('submit_prueba_43.csv') as submit:\n",
    "    perfect_csv = csv.reader(perfecto)\n",
    "    submit_csv = csv.reader(submit)\n",
    "    perf = {}\n",
    "    subm = {}\n",
    "    misseados = []\n",
    "    for id,target in perfect_csv:\n",
    "        perf[id] = perf.get(id,target)\n",
    "    for id,target in submit_csv:\n",
    "        subm[id] = subm.get(id,target)\n",
    "    for id in subm:\n",
    "        if subm[id] != perf[id]:\n",
    "            misseados.append(id)\n",
    "    for i in range(len(misseados)):\n",
    "        misseados[i] = int(misseados[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(misseados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_checkear['wrong_label'] = pa_checkear['id'].apply(lambda x: x in misseados)\n",
    "pa_checkear[pa_checkear['wrong_label'] == True].to_csv('mal_labeleados.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    287\n",
       "True     282\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa_checkear[pa_checkear['wrong_label'] == True]['text'].str.contains('http').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('mausa': virtualenv)",
   "language": "python",
   "name": "python37664bitmausavirtualenv161e16f0fd2c481895212b5856f17cf5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
