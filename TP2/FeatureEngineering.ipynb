{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\mausa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mausa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mausa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mausa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "#import xgboost as xgb\n",
    "import io\n",
    "import nltk\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "from textblob import TextBlob\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "def return_sia_compound_values(text):\n",
    "    return sia.polarity_scores(text)['compound']\n",
    "\n",
    "def remove_stopword(text):\n",
    "    new_text = []\n",
    "    for e in text:\n",
    "        if e not in stopwords and e.isalpha():\n",
    "            new_text.append(e)\n",
    "    text = new_text\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "def stemm(text):\n",
    "    text = [stemmer.stem(word) for word in text.split()]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def contains_punctuation(text):\n",
    "    punctuation = set(string.punctuation)\n",
    "    for character in text:\n",
    "        if character in punctuation:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def amount_of_punctuation(text):\n",
    "    punctuation = set(string.punctuation)\n",
    "    amount = 0\n",
    "    for character in text:\n",
    "        if character in punctuation: amount += 1\n",
    "    return amount\n",
    "\n",
    "def get_adjectives(text):\n",
    "    blob = TextBlob(text)\n",
    "    return len([word for (word,tag) in blob.tags if tag.startswith(\"JJ\")])\n",
    "\n",
    "def get_nouns(text):\n",
    "    blob = TextBlob(text)\n",
    "    return len([word for (word,tag) in blob.tags if tag.startswith(\"NN\")])\n",
    "\n",
    "def get_verbs(text):\n",
    "    blob = TextBlob(text)\n",
    "    return len([word for (word,tag) in blob.tags if tag.startswith(\"VB\")])\n",
    "\n",
    "def get_adverbs(text):\n",
    "    blob = TextBlob(text)\n",
    "    return len([word for (word,tag) in blob.tags if tag.startswith(\"RB\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "tweets['keyword'] = tweets.keyword.str.replace('%20',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7434 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7434 non-null   int64 \n",
      " 1   keyword   7378 non-null   object\n",
      " 2   location  4982 non-null   object\n",
      " 3   text      7434 non-null   object\n",
      " 4   target    7434 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 348.5+ KB\n"
     ]
    }
   ],
   "source": [
    "tweets.drop_duplicates(subset = 'text', keep = False, inplace = True)\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    3243\n",
       "True       20\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['text'].duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adición de features de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_metrics = tweets[['id','text','target']]\n",
    "tweets_metrics['text_without_stopwords'] = tweets_metrics['text'].str.split()\n",
    "tweets_metrics['text_without_stopwords'] = tweets_metrics['text_without_stopwords'].apply(remove_stopword)\n",
    "\n",
    "tweets_metrics['length'] = tweets_metrics['text'].apply(lambda x: len(x))\n",
    "tweets_metrics['avg_word_length'] = tweets_metrics['text'].str.split().apply(lambda x: [len(y) for y in x]).transform(lambda x: np.mean(x))\n",
    "tweets_metrics['amount_of_words'] = tweets_metrics['text'].str.split().transform(lambda x: len(x))\n",
    "unique_words_by_tweet = tweets_metrics['text'].transform(lambda x: x.split()).transform(lambda x: pd.Series(x).unique()).transform(lambda x: len(x))\n",
    "tweets_metrics['amount_of_unique_words'] = unique_words_by_tweet\n",
    "tweets_metrics['sentiment'] = tweets_metrics['text'].apply(lambda x: return_sia_compound_values(x))\n",
    "tweets_metrics['stopwords_count'] = tweets_metrics['text'].apply(lambda x: len([word for word in str(x).lower().split() if word in stopwords]))\n",
    "tweets_metrics['punctuation_count'] = tweets_metrics['text'].apply(lambda x: amount_of_punctuation(x))\n",
    "mentions = tweets_metrics['text'].str.findall(r'@.\\S*?(?=\\s|[:]|$)').to_frame()\n",
    "tweets_metrics['mentions_count'] = mentions['text'].apply(lambda x: len(x))\n",
    "hashtags = tweets_metrics['text'].str.findall(r'#[^?\\s].*?(?=\\s|$)')\n",
    "tweets_metrics['hashtags_count'] = hashtags.apply(lambda x: len(x))\n",
    "tweets_metrics['longest_word_length_without_stopwords'] = tweets_metrics['text_without_stopwords'].apply(lambda x: ([len(word) for word in str(x).lower().split() if not word.startswith('http')])).apply(lambda x: max(x) if len(x) > 0 else 0)\n",
    "tweets_metrics['stopword_word_ratio'] = tweets_metrics['stopwords_count'] / tweets_metrics['amount_of_words']\n",
    "\n",
    "tweets_metrics['adjectives_count'] = tweets_metrics['text'].apply(get_adjectives)\n",
    "tweets_metrics['nouns_count'] = tweets_metrics['text'].apply(get_nouns)\n",
    "tweets_metrics['verbs_count'] = tweets_metrics['text'].apply(get_verbs)\n",
    "tweets_metrics['adverbs_count'] = tweets_metrics['text'].apply(get_adverbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_metrics.to_csv('train_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = test[['id','text']]\n",
    "test_metrics['text_without_stopwords'] = test_metrics['text'].str.split()\n",
    "test_metrics['text_without_stopwords'] = test_metrics['text_without_stopwords'].apply(remove_stopword)\n",
    "\n",
    "test_metrics['length'] = test_metrics['text'].apply(lambda x: len(x))\n",
    "test_metrics['avg_word_length'] = test_metrics['text'].str.split().apply(lambda x: [len(y) for y in x]).transform(lambda x: np.mean(x))\n",
    "test_metrics['amount_of_words'] = test_metrics['text'].str.split().transform(lambda x: len(x))\n",
    "unique_words_by_tweet = test_metrics['text'].transform(lambda x: x.split()).transform(lambda x: pd.Series(x).unique()).transform(lambda x: len(x))\n",
    "test_metrics['amount_of_unique_words'] = unique_words_by_tweet\n",
    "test_metrics['sentiment'] = test_metrics['text'].apply(lambda x: return_sia_compound_values(x))\n",
    "test_metrics['stopwords_count'] = test_metrics['text'].apply(lambda x: len([word for word in str(x).lower().split() if word in stopwords]))\n",
    "test_metrics['punctuation_count'] = test_metrics['text'].apply(lambda x: amount_of_punctuation(x))\n",
    "mentions = test_metrics['text'].str.findall(r'@.\\S*?(?=\\s|[:]|$)').to_frame()\n",
    "test_metrics['mentions_count'] = mentions['text'].apply(lambda x: len(x))\n",
    "hashtags = test_metrics['text'].str.findall(r'#[^?\\s].*?(?=\\s|$)')\n",
    "test_metrics['hashtags_count'] = hashtags.apply(lambda x: len(x))\n",
    "test_metrics['longest_word_length_without_stopwords'] = test_metrics['text_without_stopwords'].apply(lambda x: ([len(word) for word in str(x).lower().split() if not word.startswith('http')])).apply(lambda x: max(x) if len(x) > 0 else 0)\n",
    "test_metrics['stopword_word_ratio'] = test_metrics['stopwords_count'] / test_metrics['amount_of_words']\n",
    "\n",
    "test_metrics['adjectives_count'] = test_metrics['text'].apply(get_adjectives)\n",
    "test_metrics['nouns_count'] = test_metrics['text'].apply(get_nouns)\n",
    "test_metrics['verbs_count'] = test_metrics['text'].apply(get_verbs)\n",
    "test_metrics['adverbs_count'] = test_metrics['text'].apply(get_adverbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics.to_csv('test_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adición de features de keyword. [Word2Vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['keyword'] = tweets['keyword'].fillna('NULL')\n",
    "test['keyword'] = test['keyword'].fillna('NULL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>null</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>null</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>null</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>null</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>null</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  keyword  id\n",
       "0    null   1\n",
       "1    null   4\n",
       "2    null   5\n",
       "3    null   6\n",
       "4    null   7"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = tweets[['keyword', 'id']]\n",
    "keywords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_tokens = keywords.keyword.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'null'"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keyword_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Parameters\n",
    "# sg ({0, 1}, optional) - Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "\n",
    "keyword_vectors = Word2Vec([keyword_tokens], min_count=1, size= 100, workers=3, window =3, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matiascano/.pyenv/versions/3.7.7/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2.5983190e-03, -6.6586147e-04, -2.3209248e-03,  1.7518990e-03,\n",
       "       -2.2995241e-03,  8.5558771e-04,  5.0506066e-04, -1.3033998e-03,\n",
       "        4.9609276e-03, -3.3725437e-03, -2.0172056e-03,  2.4199090e-03,\n",
       "        3.8925724e-03,  2.4426566e-03, -4.6416828e-03,  2.9733980e-03,\n",
       "        4.8849769e-03,  3.0582242e-03, -1.2348567e-03,  1.3275865e-03,\n",
       "       -4.0353765e-03, -2.7007733e-03, -4.7994917e-03, -1.7706358e-03,\n",
       "        7.0335745e-04, -2.8691879e-03, -2.1185733e-03,  2.5431931e-03,\n",
       "        7.8616978e-04,  3.8344462e-03,  4.2495583e-03,  2.7264808e-03,\n",
       "       -8.9071103e-04, -7.8725221e-04, -1.2452446e-03, -1.7115731e-03,\n",
       "        1.8372950e-03, -2.0356460e-03,  3.4612773e-03,  1.4445643e-03,\n",
       "        1.1148887e-03, -4.5582252e-03,  9.5972250e-04, -7.9843868e-04,\n",
       "       -5.0058635e-04, -4.2120512e-03,  4.0654014e-03,  3.1079892e-03,\n",
       "       -1.8416835e-03, -2.4908367e-03,  4.8450697e-03, -6.7695527e-04,\n",
       "       -4.5575514e-03, -3.4133701e-03, -4.7050603e-03, -1.7676153e-03,\n",
       "       -2.2063102e-03, -1.1948954e-03,  3.7763759e-03,  1.9930995e-03,\n",
       "        7.9594110e-04, -4.4226041e-03,  1.3361663e-03, -3.1914283e-03,\n",
       "        2.5891370e-04, -4.4276577e-04,  1.4528879e-03,  2.9427835e-03,\n",
       "       -3.0678094e-03,  1.0229659e-03,  2.2781312e-03,  3.4326413e-03,\n",
       "       -4.0515536e-03,  1.3632375e-03,  4.6473113e-03, -3.3594805e-03,\n",
       "        1.9867918e-03, -1.2467293e-03, -4.5596529e-03, -7.1394155e-05,\n",
       "       -5.1316508e-04,  2.9086752e-03,  2.5060595e-04, -4.1580577e-03,\n",
       "       -2.8231961e-03, -8.6636050e-04,  4.0655890e-03, -4.5452286e-03,\n",
       "        4.2583160e-03, -2.2944801e-03, -1.0343251e-03,  2.8203726e-03,\n",
       "        9.8414533e-04,  2.5825016e-03,  2.4045885e-03, -6.2124257e-04,\n",
       "        3.2580891e-03, -2.7844231e-03,  1.6930007e-03,  4.3328293e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "keyword_vectors['ablaze']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matiascano/.pyenv/versions/3.7.7/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "to_vector_matrix = {}\n",
    "\n",
    "for k in keyword_tokens:\n",
    "    to_vector_matrix[k] = keyword_vectors[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>null</td>\n",
       "      <td>-0.002577</td>\n",
       "      <td>-0.001227</td>\n",
       "      <td>-0.001444</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>-0.003910</td>\n",
       "      <td>0.003864</td>\n",
       "      <td>-0.000201</td>\n",
       "      <td>-0.002421</td>\n",
       "      <td>0.001415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004344</td>\n",
       "      <td>0.004172</td>\n",
       "      <td>0.002988</td>\n",
       "      <td>-0.001633</td>\n",
       "      <td>-0.000744</td>\n",
       "      <td>0.001731</td>\n",
       "      <td>-0.000859</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>0.002598</td>\n",
       "      <td>-0.000666</td>\n",
       "      <td>-0.002321</td>\n",
       "      <td>0.001752</td>\n",
       "      <td>-0.002300</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>-0.001303</td>\n",
       "      <td>0.004961</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001034</td>\n",
       "      <td>0.002820</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>0.002405</td>\n",
       "      <td>-0.000621</td>\n",
       "      <td>0.003258</td>\n",
       "      <td>-0.002784</td>\n",
       "      <td>0.001693</td>\n",
       "      <td>0.004333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>accident</td>\n",
       "      <td>-0.003349</td>\n",
       "      <td>0.003251</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>-0.000547</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.002836</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.000738</td>\n",
       "      <td>0.003825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001687</td>\n",
       "      <td>0.003747</td>\n",
       "      <td>0.004781</td>\n",
       "      <td>0.001277</td>\n",
       "      <td>-0.004880</td>\n",
       "      <td>0.004659</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>-0.004970</td>\n",
       "      <td>-0.000646</td>\n",
       "      <td>-0.001960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aftershock</td>\n",
       "      <td>-0.004262</td>\n",
       "      <td>0.002142</td>\n",
       "      <td>0.001838</td>\n",
       "      <td>0.002965</td>\n",
       "      <td>-0.002146</td>\n",
       "      <td>-0.001892</td>\n",
       "      <td>-0.000939</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>-0.003004</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000615</td>\n",
       "      <td>0.001087</td>\n",
       "      <td>-0.003613</td>\n",
       "      <td>-0.001999</td>\n",
       "      <td>-0.004111</td>\n",
       "      <td>0.003691</td>\n",
       "      <td>-0.000786</td>\n",
       "      <td>0.002645</td>\n",
       "      <td>0.001593</td>\n",
       "      <td>0.003497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>airplane accident</td>\n",
       "      <td>-0.001462</td>\n",
       "      <td>-0.002390</td>\n",
       "      <td>-0.003875</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>-0.002089</td>\n",
       "      <td>0.000851</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002294</td>\n",
       "      <td>-0.004053</td>\n",
       "      <td>0.004961</td>\n",
       "      <td>-0.002474</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.001964</td>\n",
       "      <td>-0.003745</td>\n",
       "      <td>-0.001532</td>\n",
       "      <td>0.002234</td>\n",
       "      <td>0.004583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>wounded</td>\n",
       "      <td>-0.001203</td>\n",
       "      <td>-0.000257</td>\n",
       "      <td>-0.001077</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.001959</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>-0.001591</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004437</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.002956</td>\n",
       "      <td>-0.002226</td>\n",
       "      <td>-0.003644</td>\n",
       "      <td>-0.000114</td>\n",
       "      <td>-0.002932</td>\n",
       "      <td>-0.000425</td>\n",
       "      <td>0.002893</td>\n",
       "      <td>0.003686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>wounds</td>\n",
       "      <td>-0.002723</td>\n",
       "      <td>-0.003479</td>\n",
       "      <td>0.004881</td>\n",
       "      <td>-0.003744</td>\n",
       "      <td>0.004931</td>\n",
       "      <td>0.003912</td>\n",
       "      <td>-0.002686</td>\n",
       "      <td>0.004263</td>\n",
       "      <td>-0.003562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004472</td>\n",
       "      <td>0.003746</td>\n",
       "      <td>-0.002081</td>\n",
       "      <td>0.002626</td>\n",
       "      <td>0.003337</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>-0.001013</td>\n",
       "      <td>-0.004865</td>\n",
       "      <td>0.004028</td>\n",
       "      <td>0.001501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>wreck</td>\n",
       "      <td>-0.004954</td>\n",
       "      <td>0.002817</td>\n",
       "      <td>-0.003835</td>\n",
       "      <td>-0.004148</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>-0.000847</td>\n",
       "      <td>-0.002482</td>\n",
       "      <td>0.004491</td>\n",
       "      <td>-0.004849</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.004922</td>\n",
       "      <td>-0.002649</td>\n",
       "      <td>-0.004030</td>\n",
       "      <td>-0.001320</td>\n",
       "      <td>-0.003117</td>\n",
       "      <td>-0.003942</td>\n",
       "      <td>-0.001964</td>\n",
       "      <td>0.002089</td>\n",
       "      <td>0.003643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>wreckage</td>\n",
       "      <td>-0.002716</td>\n",
       "      <td>0.002877</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>0.003528</td>\n",
       "      <td>-0.000802</td>\n",
       "      <td>0.001631</td>\n",
       "      <td>0.004361</td>\n",
       "      <td>0.004944</td>\n",
       "      <td>-0.003616</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004664</td>\n",
       "      <td>0.001344</td>\n",
       "      <td>-0.003860</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001464</td>\n",
       "      <td>-0.001884</td>\n",
       "      <td>0.003862</td>\n",
       "      <td>-0.004642</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>-0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>-0.000573</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>-0.003295</td>\n",
       "      <td>-0.003171</td>\n",
       "      <td>-0.001256</td>\n",
       "      <td>-0.004856</td>\n",
       "      <td>0.004678</td>\n",
       "      <td>-0.004537</td>\n",
       "      <td>-0.001206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004303</td>\n",
       "      <td>-0.004132</td>\n",
       "      <td>-0.002922</td>\n",
       "      <td>-0.003770</td>\n",
       "      <td>-0.004942</td>\n",
       "      <td>-0.001855</td>\n",
       "      <td>-0.001836</td>\n",
       "      <td>0.002531</td>\n",
       "      <td>-0.001358</td>\n",
       "      <td>0.003428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>222 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 index         0         1         2         3         4  \\\n",
       "0                 null -0.002577 -0.001227 -0.001444  0.000423 -0.003910   \n",
       "1               ablaze  0.002598 -0.000666 -0.002321  0.001752 -0.002300   \n",
       "2             accident -0.003349  0.003251  0.004571 -0.000547  0.002326   \n",
       "3           aftershock -0.004262  0.002142  0.001838  0.002965 -0.002146   \n",
       "4    airplane accident -0.001462 -0.002390 -0.003875  0.001232  0.002163   \n",
       "..                 ...       ...       ...       ...       ...       ...   \n",
       "217            wounded -0.001203 -0.000257 -0.001077  0.000290  0.000039   \n",
       "218             wounds -0.002723 -0.003479  0.004881 -0.003744  0.004931   \n",
       "219              wreck -0.004954  0.002817 -0.003835 -0.004148  0.000410   \n",
       "220           wreckage -0.002716  0.002877  0.000625  0.003528 -0.000802   \n",
       "221            wrecked -0.000573  0.000264 -0.003295 -0.003171 -0.001256   \n",
       "\n",
       "            5         6         7         8  ...        90        91  \\\n",
       "0    0.003864 -0.000201 -0.002421  0.001415  ...  0.004344  0.004172   \n",
       "1    0.000856  0.000505 -0.001303  0.004961  ... -0.001034  0.002820   \n",
       "2    0.002836  0.000025 -0.000738  0.003825  ...  0.001687  0.003747   \n",
       "3   -0.001892 -0.000939  0.001145 -0.003004  ... -0.000615  0.001087   \n",
       "4   -0.002089  0.000851  0.000408  0.000895  ... -0.002294 -0.004053   \n",
       "..        ...       ...       ...       ...  ...       ...       ...   \n",
       "217  0.000181  0.001959  0.000600 -0.001591  ... -0.004437  0.001071   \n",
       "218  0.003912 -0.002686  0.004263 -0.003562  ...  0.004472  0.003746   \n",
       "219 -0.000847 -0.002482  0.004491 -0.004849  ...  0.000610  0.004922   \n",
       "220  0.001631  0.004361  0.004944 -0.003616  ... -0.004664  0.001344   \n",
       "221 -0.004856  0.004678 -0.004537 -0.001206  ...  0.004303 -0.004132   \n",
       "\n",
       "           92        93        94        95        96        97        98  \\\n",
       "0    0.002988 -0.001633 -0.000744  0.001731 -0.000859  0.000612  0.000783   \n",
       "1    0.000984  0.002583  0.002405 -0.000621  0.003258 -0.002784  0.001693   \n",
       "2    0.004781  0.001277 -0.004880  0.004659  0.000265 -0.004970 -0.000646   \n",
       "3   -0.003613 -0.001999 -0.004111  0.003691 -0.000786  0.002645  0.001593   \n",
       "4    0.004961 -0.002474  0.000193  0.001964 -0.003745 -0.001532  0.002234   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "217  0.002956 -0.002226 -0.003644 -0.000114 -0.002932 -0.000425  0.002893   \n",
       "218 -0.002081  0.002626  0.003337  0.000401 -0.001013 -0.004865  0.004028   \n",
       "219 -0.002649 -0.004030 -0.001320 -0.003117 -0.003942 -0.001964  0.002089   \n",
       "220 -0.003860  0.001019  0.001464 -0.001884  0.003862 -0.004642  0.000096   \n",
       "221 -0.002922 -0.003770 -0.004942 -0.001855 -0.001836  0.002531 -0.001358   \n",
       "\n",
       "           99  \n",
       "0    0.000369  \n",
       "1    0.004333  \n",
       "2   -0.001960  \n",
       "3    0.003497  \n",
       "4    0.004583  \n",
       "..        ...  \n",
       "217  0.003686  \n",
       "218  0.001501  \n",
       "219  0.003643  \n",
       "220 -0.000189  \n",
       "221  0.003428  \n",
       "\n",
       "[222 rows x 101 columns]"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_w2v = pd.DataFrame.from_dict(to_vector_matrix).T.reset_index()\n",
    "keyword_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux = []\n",
    "aux.append('keyword')\n",
    "for i in range (0, 100):\n",
    "    name = 'v' + str(i)\n",
    "    aux.append(name)\n",
    "len(aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_w2v.columns = aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>v0</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>...</th>\n",
       "      <th>v90</th>\n",
       "      <th>v91</th>\n",
       "      <th>v92</th>\n",
       "      <th>v93</th>\n",
       "      <th>v94</th>\n",
       "      <th>v95</th>\n",
       "      <th>v96</th>\n",
       "      <th>v97</th>\n",
       "      <th>v98</th>\n",
       "      <th>v99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>null</td>\n",
       "      <td>-0.002577</td>\n",
       "      <td>-0.001227</td>\n",
       "      <td>-0.001444</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>-0.003910</td>\n",
       "      <td>0.003864</td>\n",
       "      <td>-0.000201</td>\n",
       "      <td>-0.002421</td>\n",
       "      <td>0.001415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004344</td>\n",
       "      <td>0.004172</td>\n",
       "      <td>0.002988</td>\n",
       "      <td>-0.001633</td>\n",
       "      <td>-0.000744</td>\n",
       "      <td>0.001731</td>\n",
       "      <td>-0.000859</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>0.002598</td>\n",
       "      <td>-0.000666</td>\n",
       "      <td>-0.002321</td>\n",
       "      <td>0.001752</td>\n",
       "      <td>-0.002300</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>-0.001303</td>\n",
       "      <td>0.004961</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001034</td>\n",
       "      <td>0.002820</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>0.002405</td>\n",
       "      <td>-0.000621</td>\n",
       "      <td>0.003258</td>\n",
       "      <td>-0.002784</td>\n",
       "      <td>0.001693</td>\n",
       "      <td>0.004333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>accident</td>\n",
       "      <td>-0.003349</td>\n",
       "      <td>0.003251</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>-0.000547</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.002836</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.000738</td>\n",
       "      <td>0.003825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001687</td>\n",
       "      <td>0.003747</td>\n",
       "      <td>0.004781</td>\n",
       "      <td>0.001277</td>\n",
       "      <td>-0.004880</td>\n",
       "      <td>0.004659</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>-0.004970</td>\n",
       "      <td>-0.000646</td>\n",
       "      <td>-0.001960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aftershock</td>\n",
       "      <td>-0.004262</td>\n",
       "      <td>0.002142</td>\n",
       "      <td>0.001838</td>\n",
       "      <td>0.002965</td>\n",
       "      <td>-0.002146</td>\n",
       "      <td>-0.001892</td>\n",
       "      <td>-0.000939</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>-0.003004</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000615</td>\n",
       "      <td>0.001087</td>\n",
       "      <td>-0.003613</td>\n",
       "      <td>-0.001999</td>\n",
       "      <td>-0.004111</td>\n",
       "      <td>0.003691</td>\n",
       "      <td>-0.000786</td>\n",
       "      <td>0.002645</td>\n",
       "      <td>0.001593</td>\n",
       "      <td>0.003497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>airplane accident</td>\n",
       "      <td>-0.001462</td>\n",
       "      <td>-0.002390</td>\n",
       "      <td>-0.003875</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>-0.002089</td>\n",
       "      <td>0.000851</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002294</td>\n",
       "      <td>-0.004053</td>\n",
       "      <td>0.004961</td>\n",
       "      <td>-0.002474</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.001964</td>\n",
       "      <td>-0.003745</td>\n",
       "      <td>-0.001532</td>\n",
       "      <td>0.002234</td>\n",
       "      <td>0.004583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             keyword        v0        v1        v2        v3        v4  \\\n",
       "0               null -0.002577 -0.001227 -0.001444  0.000423 -0.003910   \n",
       "1             ablaze  0.002598 -0.000666 -0.002321  0.001752 -0.002300   \n",
       "2           accident -0.003349  0.003251  0.004571 -0.000547  0.002326   \n",
       "3         aftershock -0.004262  0.002142  0.001838  0.002965 -0.002146   \n",
       "4  airplane accident -0.001462 -0.002390 -0.003875  0.001232  0.002163   \n",
       "\n",
       "         v5        v6        v7        v8  ...       v90       v91       v92  \\\n",
       "0  0.003864 -0.000201 -0.002421  0.001415  ...  0.004344  0.004172  0.002988   \n",
       "1  0.000856  0.000505 -0.001303  0.004961  ... -0.001034  0.002820  0.000984   \n",
       "2  0.002836  0.000025 -0.000738  0.003825  ...  0.001687  0.003747  0.004781   \n",
       "3 -0.001892 -0.000939  0.001145 -0.003004  ... -0.000615  0.001087 -0.003613   \n",
       "4 -0.002089  0.000851  0.000408  0.000895  ... -0.002294 -0.004053  0.004961   \n",
       "\n",
       "        v93       v94       v95       v96       v97       v98       v99  \n",
       "0 -0.001633 -0.000744  0.001731 -0.000859  0.000612  0.000783  0.000369  \n",
       "1  0.002583  0.002405 -0.000621  0.003258 -0.002784  0.001693  0.004333  \n",
       "2  0.001277 -0.004880  0.004659  0.000265 -0.004970 -0.000646 -0.001960  \n",
       "3 -0.001999 -0.004111  0.003691 -0.000786  0.002645  0.001593  0.003497  \n",
       "4 -0.002474  0.000193  0.001964 -0.003745 -0.001532  0.002234  0.004583  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_w2v.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_w2v.to_csv('keyword_w2v_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordenadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = pd.read_csv(\"../TP1/locations.csv\", usecols=['location', 'point'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations.fillna('null', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_loc = locations.loc[0, 'location']\n",
    "locations.replace(empty_loc, 'null', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>point</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>glasgow</td>\n",
       "      <td>(55.8609825, -4.2488787, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>melbourne, australia</td>\n",
       "      <td>(-37.8142176, 144.9631608, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>news</td>\n",
       "      <td>(49.04172215, -122.27255349013137, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alberta</td>\n",
       "      <td>(55.001251, -115.002136, 0.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 location                                    point\n",
       "0                    null                                     null\n",
       "1                glasgow             (55.8609825, -4.2488787, 0.0)\n",
       "2    melbourne, australia          (-37.8142176, 144.9631608, 0.0)\n",
       "3                    news  (49.04172215, -122.27255349013137, 0.0)\n",
       "4                 alberta            (55.001251, -115.002136, 0.0)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_to_list(point):\n",
    "    if point == 'null':\n",
    "        return [float('inf'), float('inf')]\n",
    "    \n",
    "    coordinates = []\n",
    "    aux = point[:]\n",
    "    row = aux.strip( '()' ).split(',')\n",
    "    coordinates.append(float(row[0]))\n",
    "    coordinates.append(float(row[1]))\n",
    "    return coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations['point'] = locations.point.apply(point_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = locations.point.apply(pd.Series)\n",
    "aux.columns = ['x', 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations['x'] = aux['x']\n",
    "locations['y'] = aux['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>point</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>null</td>\n",
       "      <td>[inf, inf]</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>glasgow</td>\n",
       "      <td>[55.8609825, -4.2488787]</td>\n",
       "      <td>55.860982</td>\n",
       "      <td>-4.248879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>melbourne, australia</td>\n",
       "      <td>[-37.8142176, 144.9631608]</td>\n",
       "      <td>-37.814218</td>\n",
       "      <td>144.963161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>news</td>\n",
       "      <td>[49.04172215, -122.27255349013137]</td>\n",
       "      <td>49.041722</td>\n",
       "      <td>-122.272553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alberta</td>\n",
       "      <td>[55.001251, -115.002136]</td>\n",
       "      <td>55.001251</td>\n",
       "      <td>-115.002136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 location                               point          x  \\\n",
       "0                    null                          [inf, inf]        inf   \n",
       "1                glasgow             [55.8609825, -4.2488787]  55.860982   \n",
       "2    melbourne, australia          [-37.8142176, 144.9631608] -37.814218   \n",
       "3                    news  [49.04172215, -122.27255349013137]  49.041722   \n",
       "4                 alberta            [55.001251, -115.002136]  55.001251   \n",
       "\n",
       "            y  \n",
       "0         inf  \n",
       "1   -4.248879  \n",
       "2  144.963161  \n",
       "3 -122.272553  \n",
       "4 -115.002136  "
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['location'] = tweets['location'].apply(str.lower)\n",
    "test['location'] = test['location'].apply(str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates_train = tweets.merge(locations.loc[:, ['location', 'x', 'y']], left_on='location', right_on='location', how='left').loc[:, ['id', 'x', 'y']]\n",
    "coordinates_test = test.merge(locations.loc[:, ['location', 'x', 'y']], left_on='location', right_on='location', how='left').loc[:, ['id', 'x', 'y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates_train.fillna(float('inf'), inplace=True)\n",
    "coordinates_test.fillna(float('inf'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_with_xy = tweets_metrics.merge(coordinates_train, left_on='id', right_on='id')\n",
    "test_metrics_with_xy = test_metrics.merge(coordinates_test, left_on='id', right_on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_with_xy.to_csv('train_features_xy.csv', index=False)\n",
    "test_metrics_with_xy.to_csv('test_features_xy.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1477,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['keyword'] = tweets.keyword.str.replace('%20',' ')\n",
    "tweets.keyword.fillna('null', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disaster_list = list(tweets['keyword'].unique())\n",
    "len(disaster_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "crash = ['collide', 'collided', 'collision', 'crash', 'crashed', 'wreck', 'wreckage', 'wrecked']\n",
    "\n",
    "emergency = ['emergency', 'emergency plan']\n",
    "\n",
    "electricity = ['electrocute', 'electrocuted',]\n",
    "\n",
    "helpers = ['ambulance', 'police', 'siren', 'sirens', 'emergency services', 'first responders',\\\n",
    "           'stretcher', 'eyewitness', 'rescuers']\n",
    "\n",
    "panic = ['screamed', 'screaming', 'screams', 'panic', 'mayhem', 'riot', 'rioting', 'fear', 'panicking', 'trauma',\\\n",
    "         'trouble', 'hail', 'pandemonium']\n",
    "\n",
    "hostages = ['hostage', 'hostages', 'trapped']\n",
    "\n",
    "quarentine = ['quarantine', 'quarantined']\n",
    "\n",
    "colapse = ['bridge collapse', 'collapse', 'collapsed', 'demolish', 'demolished', 'demolition', 'structural failure']\n",
    "\n",
    "accident = ['accident', 'airplane accident', 'derail', 'derailed', 'derailment', 'oil spill']\n",
    "\n",
    "fire = ['ablaze', 'hellfire', 'smoke', 'wild fires', 'wildfire', 'buildings burning',\\\n",
    "        'buildings on fire', 'burned', 'burning', 'burning buildings', 'bush fires', 'fire',\\\n",
    "        'fire truck', 'flames', 'forest fire', 'forest fires', 'blaze', 'blazing', 'arson', 'arsonist']\n",
    "\n",
    "nuclear = ['nuclear disaster', 'nuclear reactor', 'radiation emergency', 'meltdown']\n",
    "\n",
    "explotion = ['explode', 'exploded', 'explosion', 'blown up', 'blew up', 'loud bang']\n",
    "\n",
    "survivor = ['survive', 'survived', 'rescue', 'rescued', 'survivors', 'evacuate', 'evacuated', 'evacuation', 'refugees']\n",
    "\n",
    "wounded = ['wounded', 'wounds', 'bleeding', 'bloody', 'injured', 'injuries', 'injury', 'traumatised', 'blood']\n",
    "\n",
    "bomb = ['suicide bomb', 'suicide bomber', 'suicide bombing', 'bomb', 'bombed', 'bombing', 'detonate', 'detonation']\n",
    "\n",
    "storm = ['storm', 'thunderstorm', 'thunder', 'rainstorm', 'violent storm', 'windstorm', 'lightning', 'hailstorm']\n",
    "\n",
    "water = ['flood', 'flooding', 'floods', 'inundated', 'inundation', 'sinking', 'drown', 'drowned', 'drowning', 'sunk']\n",
    "\n",
    "natural_disaster = ['heat wave','sandstorm', 'seismic' ,'avalanche', 'tsunami', 'twister',\\\n",
    "                    'typhoon',  'tornado', 'hurricane', 'natural disaster', 'cyclone', 'volcano',\\\n",
    "                    'drought', 'dust storm', 'earthquake',  'lava', 'aftershock', 'snowstorm', 'blizzard',\\\n",
    "                    'whirlwind', 'upheaval',  'landslide', 'cliff fall', 'mudslide', 'sinkhole', 'displaced',\\\n",
    "                    'epicentre']\n",
    "\n",
    "attack = ['attack', 'attacked']\n",
    "\n",
    "casualties = ['mass murder', 'mass murderer', 'massacre', 'fatal', 'fatalities', 'fatality', 'casualties',\\\n",
    "              'casualty', 'body bag', 'body bagging', 'body bags', 'dead', 'death', 'deaths',  'tragedy']\n",
    "\n",
    "terrorism = ['terrorism', 'terrorist', 'threat', 'hijack', 'hijacker', 'hijacking', 'bioterror', 'bioterrorism']\n",
    "\n",
    "destruction = ['destroyed', 'destruction', 'devastated',\\\n",
    "               'devastation', 'disaster', 'annihilated', 'annihilation', 'apocalypse',\\\n",
    "               'armageddon', 'catastrophe', 'catastrophic', 'obliterate', 'obliterated',\\\n",
    "               'obliteration', 'damage', 'destroy', 'desolate', 'desolation', 'blight',\\\n",
    "               'harm', 'hazard', 'hazardous', 'danger', 'ruin', 'engulfed', 'rubble', 'debris',\\\n",
    "               'razed', 'flattened', 'crush', 'crushed']\n",
    "\n",
    "warlike = ['war zone', 'weapon', 'weapons', 'military', 'army', 'battle', 'outbreak', 'chemical emergency', 'curfew']\n",
    "\n",
    "starvation = ['famine', 'deluge', 'deluged']\n",
    "\n",
    "\n",
    "null = ['null']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "disasters = [crash, emergency, electricity, helpers, panic, hostages, quarentine, colapse, accident, fire,\\\n",
    "             nuclear, explotion, survivor, wounded, bomb, storm, water, natural_disaster, attack, casualties,\\\n",
    "             terrorism, destruction, warlike, starvation, null]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for l in disasters:\n",
    "    count += len(l)\n",
    "count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
